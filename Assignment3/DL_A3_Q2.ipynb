{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_A3_Q2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "uRGyFIT_axmv",
        "NlX_ycI-a3cD",
        "Am94nfPXpzI0"
      ],
      "authorship_tag": "ABX9TyMmo4Y6ib09da28HmiUVf21"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "913a5a619fa24dd6ac8986fc1a193d31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_13621a377b224488b289d6f01b40e626",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2d874f98a47e45d2b9225c2ea396cc39",
              "IPY_MODEL_830fa0af1c264fd79cd7057a4142f125"
            ]
          }
        },
        "71f32850b58e4e9da1ee7a16c43dc59e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1078a521bc4844c9b3e73392f14606bf",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_76aa5e65d72a460ba5abd744b435fc84",
              "IPY_MODEL_a09acf355cd54d2180127a2b0a827767"
            ]
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRGyFIT_axmv"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IntdRGraoSN",
        "outputId": "95f6fbfe-fd62-4721-a8de-fd2f04583614"
      },
      "source": [
        "!wget https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
        "!tar -xvf  'dakshina_dataset_v1.0.tar'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-25 15:35:27--  https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.142.128, 74.125.195.128, 74.125.197.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.142.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2008340480 (1.9G) [application/x-tar]\n",
            "Saving to: ‘dakshina_dataset_v1.0.tar’\n",
            "\n",
            "dakshina_dataset_v1 100%[===================>]   1.87G  42.8MB/s    in 18s     \n",
            "\n",
            "2021-05-25 15:35:45 (106 MB/s) - ‘dakshina_dataset_v1.0.tar’ saved [2008340480/2008340480]\n",
            "\n",
            "dakshina_dataset_v1.0/bn/\n",
            "dakshina_dataset_v1.0/bn/lexicons/\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/romanized/\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/gu/\n",
            "dakshina_dataset_v1.0/gu/lexicons/\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/romanized/\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/hi/\n",
            "dakshina_dataset_v1.0/hi/lexicons/\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/hi/romanized/\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/kn/\n",
            "dakshina_dataset_v1.0/kn/lexicons/\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/kn/romanized/\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ml/\n",
            "dakshina_dataset_v1.0/ml/lexicons/\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/tmp.rom.txt\n",
            "dakshina_dataset_v1.0/ml/lexicons/tmp.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/romanized/\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/mr/\n",
            "dakshina_dataset_v1.0/mr/lexicons/\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/romanized/\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/pa/\n",
            "dakshina_dataset_v1.0/pa/lexicons/\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/pa/romanized/\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/sd/\n",
            "dakshina_dataset_v1.0/sd/lexicons/\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/sd/romanized/\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/si/\n",
            "dakshina_dataset_v1.0/si/lexicons/\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/romanized/\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ta/\n",
            "dakshina_dataset_v1.0/ta/lexicons/\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/romanized/\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/te/\n",
            "dakshina_dataset_v1.0/te/lexicons/\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/romanized/\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ur/\n",
            "dakshina_dataset_v1.0/ur/lexicons/\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/romanized/\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/README.md\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlX_ycI-a3cD"
      },
      "source": [
        "# load and create model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jyB-MXYatP8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac37b722-b579-4d38-8e36-898ffbf18ebc"
      },
      "source": [
        "%pip install wandb -q\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import pandas as pd\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.8MB 7.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 174kB 48.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 11.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 47.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 9.1MB/s \n",
            "\u001b[?25h  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-9k_tSca8-P"
      },
      "source": [
        "input_token_index = None\n",
        "target_token_index = None\n",
        "MAX_LEN_input = None\n",
        "MAX_LEN_target = None\n",
        "num_encoder_tokens = 30\n",
        "num_decoder_tokens = 70\n",
        "input_tokenizer = None\n",
        "target_tokenizer = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4KPNp7GbBZW"
      },
      "source": [
        "def tokenize(data,vocab_size):\n",
        "  tokenizer = Tokenizer(num_words=vocab_size,char_level=True)\n",
        "  tokenizer.fit_on_texts(data)\n",
        "  temp=tokenizer.texts_to_sequences(data)\n",
        "  # print(data[:3])\n",
        "  # print(temp[:3])\n",
        "  dictionary = tokenizer.word_index\n",
        "  return temp , dictionary , tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qacRMXz9bCPu"
      },
      "source": [
        "def load_and_preprocess():\n",
        "  global input_token_index , target_token_index , MAX_LEN_input , MAX_LEN_target ,num_decoder_tokens,num_encoder_tokens , input_tokenizer , target_tokenizer\n",
        "  data_path = 'dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.train.tsv'\n",
        "  input_texts = []\n",
        "  target_texts = []\n",
        "  with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.read().split(\"\\n\")\n",
        "  for line in lines[:-1]:\n",
        "    temp = line.split('\\t')\n",
        "    input_text, target_text = temp[1],temp[0] \n",
        "    target_text = \"\\t\" + target_text + \"\\n\"\n",
        "    input_text = input_text+\"\\n\"\n",
        "\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "  \n",
        "  MAX_LEN_input = max([len(txt) for txt in input_texts])\n",
        "  MAX_LEN_target = max([len(txt) for txt in target_texts])\n",
        "\n",
        "  # tokenize\n",
        "  encoder_input , input_token_index , input_tokenizer = tokenize(input_texts , num_encoder_tokens)\n",
        "  decoder_input , target_token_index, target_tokenizer = tokenize(target_texts , num_decoder_tokens) \n",
        "\n",
        "  # padding\n",
        "  encoder_input_data = pad_sequences(encoder_input, maxlen=MAX_LEN_input, dtype='int32', padding='post', truncating='post',value= input_token_index[\"\\n\"])\n",
        "  decoder_input_data = pad_sequences(decoder_input, maxlen=MAX_LEN_target, dtype='int32', padding='post', truncating='post',value=target_token_index[\"\\n\"])\n",
        "\n",
        "  decoder_target_data = np.zeros((len(input_texts), MAX_LEN_target, num_decoder_tokens), dtype=\"float32\")\n",
        "  for i,  target_text in enumerate(target_texts):\n",
        "    for t, char in enumerate(target_text):\n",
        "      if t > 0:\n",
        "        decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "    decoder_target_data[i, t:, target_token_index[\"\\n\"]] = 1.0\n",
        "\n",
        "  return encoder_input_data , decoder_input_data, decoder_target_data\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPHBmJHhKsRG"
      },
      "source": [
        "def load_val_data(data_path = 'dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.dev.tsv'):\n",
        "  global input_token_index , target_token_index , MAX_LEN_input , MAX_LEN_target ,num_decoder_tokens,num_encoder_tokens , input_tokenizer , target_tokenizer\n",
        "  \n",
        "  input_texts = []\n",
        "  target_texts = []\n",
        "  with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.read().split(\"\\n\")\n",
        "  for line in lines[:-1]:\n",
        "    temp = line.split('\\t')\n",
        "    target_text , input_text = temp[0],temp[1] \n",
        "    target_text = \"\\t\" + target_text + \"\\n\"\n",
        "    input_text = input_text+\"\\n\"\n",
        "\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "  \n",
        "  # tokenize\n",
        "  encoder_input  = input_tokenizer.texts_to_sequences(input_texts)\n",
        "  decoder_input  = target_tokenizer.texts_to_sequences(target_texts) \n",
        "\n",
        "  # padding\n",
        "  encoder_input_data = pad_sequences(encoder_input, maxlen=MAX_LEN_input, dtype='int32', padding='post', truncating='post',value= input_token_index[\"\\n\"])\n",
        "  decoder_input_data = pad_sequences(decoder_input, maxlen=MAX_LEN_target, dtype='int32', padding='post', truncating='post',value=target_token_index[\"\\n\"])\n",
        "\n",
        "  decoder_target_data = np.zeros((len(input_texts), MAX_LEN_target, num_decoder_tokens), dtype=\"float32\")\n",
        "  for i,  target_text in enumerate(target_texts):\n",
        "    for t, char in enumerate(target_text):\n",
        "      if char == 'ૠ':\n",
        "        char = 'ઋ'\n",
        "      if t > 0:\n",
        "        decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "    decoder_target_data[i, t:, target_token_index[\"\\n\"]] = 1.0\n",
        "\n",
        "  return encoder_input_data , decoder_input_data, decoder_target_data\n",
        "\n",
        "# load_val_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqmDJPwWbH0I"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense,LSTM,Input,GRU,SimpleRNN,Dropout,Embedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sd0yQw9ubKFK"
      },
      "source": [
        "def create_model(m_name=\"LSTM\",n_e_layers=1,n_d_layers=1,latent_dim = 100,embedding_size = 16,dropout = 0 , recurrent_dropout = 0):\n",
        "  keras.backend.clear_session()\n",
        "  # num_encoder_tokens = 30\n",
        "  # num_decoder_tokens = 50\n",
        "  # Define an input sequence and process it.\n",
        "  input1 = Input(shape=(None,),name= \"input_1\")\n",
        "  encoder_inputs = Embedding(input_dim = num_encoder_tokens, output_dim = embedding_size)(input1)\n",
        "\n",
        "  encoder = globals()[m_name](latent_dim,dropout=dropout,recurrent_dropout = recurrent_dropout, return_state=True,return_sequences=True)\n",
        "  e_o = encoder(encoder_inputs)\n",
        "  prev = e_o\n",
        "  for i in range(1,n_e_layers):\n",
        "    e = globals()[m_name](latent_dim, dropout=dropout,recurrent_dropout = recurrent_dropout,return_state=True,return_sequences=True)\n",
        "    e_o = e(prev[0])\n",
        "    prev = e_o\n",
        "  \n",
        "  input2 = Input(shape=(None,),name=\"input_2\")\n",
        "  decoder_inputs = Embedding(input_dim = num_decoder_tokens, output_dim = embedding_size)(input2)\n",
        "  d_l = globals()[m_name](latent_dim,dropout=dropout,recurrent_dropout = recurrent_dropout, return_sequences=True, return_state=True)(decoder_inputs,initial_state = e_o[1:])\n",
        "  p_d = d_l[0]\n",
        "  for i in range(1,n_d_layers):\n",
        "    d_l = globals()[m_name](latent_dim,dropout=dropout,recurrent_dropout = recurrent_dropout, return_state=True, return_sequences=True)(p_d,initial_state = e_o[1:])\n",
        "    p_d = d_l[0]\n",
        "\n",
        "  \n",
        "  decoder_dense = Dense(num_decoder_tokens, activation=\"softmax\")\n",
        "  decoder_outputs = decoder_dense(d_l[0])\n",
        "\n",
        "  # Define the model that will turn\n",
        "  # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "  model = keras.Model([input1,input2], decoder_outputs)\n",
        "\n",
        "  return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Am94nfPXpzI0"
      },
      "source": [
        "# sweep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M21Ip4O-bNDb"
      },
      "source": [
        "def train():\n",
        "  run = wandb.init()\n",
        "  c = run.config\n",
        "  name = \"model_\"+c.model+\"_o_\"+c.optimizer+\"_el_\"+str(c.encoder_layers)+\"_dl_\"+str(c.decoder_layers)+\"_hs_\"+str(c.hidden_size)+\"_em_\"+str(c.embedding_size)+\"_d_\"+str(c.dropout)\n",
        "  run.name = name\n",
        "  print(name)\n",
        "  batch_size = 128\n",
        "  epochs = 20\n",
        "\n",
        "  encoder_input_data,decoder_input_data ,decoder_target_data = load_and_preprocess()\n",
        "  val_encoder_input_data,val_decoder_input_data ,val_decoder_target_data = load_val_data()\n",
        "  es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
        "\n",
        "  model = create_model(c.model,c.encoder_layers,c.decoder_layers,c.hidden_size,c.embedding_size,c.dropout,0)\n",
        "  model.compile(optimizer=c.optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "  model.fit(\n",
        "    [encoder_input_data, decoder_input_data],\n",
        "    decoder_target_data,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=([val_encoder_input_data, val_decoder_input_data],val_decoder_target_data),\n",
        "    # validation_split=0.2,\n",
        "    callbacks=[WandbCallback(),es]\n",
        "  )\n",
        "  temp = model.predict([val_encoder_input_data, val_decoder_input_data]).argmax(axis=-1)\n",
        "  val_word_acc = sum((temp[:,:-1] == val_decoder_input_data[:,1:]).all(axis=-1)) / len(val_encoder_input_data)\n",
        "  # print(val_word_acc)\n",
        "\n",
        "  temp = model.predict([encoder_input_data,decoder_input_data]).argmax(axis=-1)\n",
        "  train_word_acc = sum((temp[:,:-1] == decoder_input_data[:,1:]).all(axis=-1)) / len(encoder_input_data)\n",
        "  # print(train_word_acc)\n",
        "\n",
        "  wandb.log({\"val_word_acc\" : round(val_word_acc,4) , \"train_word_acc\" : round(train_word_acc,4)})\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBLtbSTMeV4W"
      },
      "source": [
        "sweep_config={\n",
        "    'method' : 'random' ,\n",
        "    'metric' : { 'name' : 'val_word_acc' , 'goal' : 'maximize' } ,\n",
        "    'parameters' : {\n",
        "        'model' : { 'values' : ['LSTM','GRU','SimpleRNN'] },\n",
        "        'dropout' : { 'values' : [0.1,0.2]},\n",
        "        'embedding_size' : {'values' : [64,128]},\n",
        "        'encoder_layers' : {'values' : [1,2,3]},\n",
        "        'decoder_layers' : {'values' : [1,2,3]},\n",
        "        'hidden_size' : {'values' : [128,256,512]},\n",
        "        'optimizer' : {'values' : ['rmsprop' ,'adam']}\n",
        "    }\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "913a5a619fa24dd6ac8986fc1a193d31",
            "71f32850b58e4e9da1ee7a16c43dc59e",
            "13621a377b224488b289d6f01b40e626",
            "2d874f98a47e45d2b9225c2ea396cc39",
            "830fa0af1c264fd79cd7057a4142f125",
            "1078a521bc4844c9b3e73392f14606bf",
            "76aa5e65d72a460ba5abd744b435fc84",
            "a09acf355cd54d2180127a2b0a827767"
          ]
        },
        "id": "MXcJYQLriFNT",
        "outputId": "eb46714b-af23-4e2e-ab6c-e660a921e4ec"
      },
      "source": [
        "\n",
        "sweepid = wandb.sweep(sweep_config,project=\"DL_A3_Q2_final\",entity =\"sonagara\")\n",
        "wandb.agent(sweepid,train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "wandb: Paste an API key from your profile and hit enter: ··········\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Create sweep with ID: u6pg8nh7\n",
            "Sweep URL: https://wandb.ai/sonagara/DL_A3_Q1_testing1/sweeps/u6pg8nh7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: m5v41xos with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_layers: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_layers: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel: SimpleRNN\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msonagara\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.30<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">scarlet-sweep-1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/sonagara/DL_A3_Q1_testing1\" target=\"_blank\">https://wandb.ai/sonagara/DL_A3_Q1_testing1</a><br/>\n",
              "                Sweep page: <a href=\"https://wandb.ai/sonagara/DL_A3_Q1_testing1/sweeps/u6pg8nh7\" target=\"_blank\">https://wandb.ai/sonagara/DL_A3_Q1_testing1/sweeps/u6pg8nh7</a><br/>\n",
              "Run page: <a href=\"https://wandb.ai/sonagara/DL_A3_Q1_testing1/runs/m5v41xos\" target=\"_blank\">https://wandb.ai/sonagara/DL_A3_Q1_testing1/runs/m5v41xos</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210517_114518-m5v41xos</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "model_SimpleRNN_o_adam_el_2_dl_1_hs_256_em_128_d_0.2\n",
            "823/823 [==============================] - 90s 105ms/step - loss: 1.2063 - accuracy: 0.6967 - val_loss: 0.9500 - val_accuracy: 0.7355\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 705<br/>Program ended successfully."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "913a5a619fa24dd6ac8986fc1a193d31",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 4.18MB of 4.18MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find user logs for this run at: <code>/content/wandb/run-20210517_114518-m5v41xos/logs/debug.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find internal logs for this run at: <code>/content/wandb/run-20210517_114518-m5v41xos/logs/debug-internal.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run summary:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>1.05199</td></tr><tr><td>accuracy</td><td>0.71678</td></tr><tr><td>val_loss</td><td>0.94997</td></tr><tr><td>val_accuracy</td><td>0.73554</td></tr><tr><td>_runtime</td><td>133</td></tr><tr><td>_timestamp</td><td>1621252051</td></tr><tr><td>_step</td><td>1</td></tr><tr><td>best_val_loss</td><td>0.94997</td></tr><tr><td>best_epoch</td><td>0</td></tr><tr><td>val_word_acc</td><td>0.0</td></tr><tr><td>train_word_acc</td><td>0.0</td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run history:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>epoch</td><td>▁</td></tr><tr><td>loss</td><td>▁</td></tr><tr><td>accuracy</td><td>▁</td></tr><tr><td>val_loss</td><td>▁</td></tr><tr><td>val_accuracy</td><td>▁</td></tr><tr><td>_runtime</td><td>▁█</td></tr><tr><td>_timestamp</td><td>▁█</td></tr><tr><td>_step</td><td>▁█</td></tr><tr><td>val_word_acc</td><td>▁</td></tr><tr><td>train_word_acc</td><td>▁</td></tr></table><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    <br/>Synced <strong style=\"color:#cdcd00\">scarlet-sweep-1</strong>: <a href=\"https://wandb.ai/sonagara/DL_A3_Q1_testing1/runs/m5v41xos\" target=\"_blank\">https://wandb.ai/sonagara/DL_A3_Q1_testing1/runs/m5v41xos</a><br/>\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: gkkkojcj with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel: LSTM\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: rmsprop\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.30<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">silvery-sweep-2</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/sonagara/DL_A3_Q1_testing1\" target=\"_blank\">https://wandb.ai/sonagara/DL_A3_Q1_testing1</a><br/>\n",
              "                Sweep page: <a href=\"https://wandb.ai/sonagara/DL_A3_Q1_testing1/sweeps/u6pg8nh7\" target=\"_blank\">https://wandb.ai/sonagara/DL_A3_Q1_testing1/sweeps/u6pg8nh7</a><br/>\n",
              "Run page: <a href=\"https://wandb.ai/sonagara/DL_A3_Q1_testing1/runs/gkkkojcj\" target=\"_blank\">https://wandb.ai/sonagara/DL_A3_Q1_testing1/runs/gkkkojcj</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210517_114737-gkkkojcj</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "model_LSTM_o_rmsprop_el_3_dl_3_hs_64_em_128_d_0.2\n",
            "823/823 [==============================] - 64s 65ms/step - loss: 1.3277 - accuracy: 0.6870 - val_loss: 0.9235 - val_accuracy: 0.7436\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 779<br/>Program ended successfully."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "71f32850b58e4e9da1ee7a16c43dc59e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 1.99MB of 1.99MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find user logs for this run at: <code>/content/wandb/run-20210517_114737-gkkkojcj/logs/debug.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find internal logs for this run at: <code>/content/wandb/run-20210517_114737-gkkkojcj/logs/debug-internal.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run summary:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>1.1351</td></tr><tr><td>accuracy</td><td>0.70639</td></tr><tr><td>val_loss</td><td>0.92348</td></tr><tr><td>val_accuracy</td><td>0.74362</td></tr><tr><td>_runtime</td><td>135</td></tr><tr><td>_timestamp</td><td>1621252192</td></tr><tr><td>_step</td><td>1</td></tr><tr><td>best_val_loss</td><td>0.92348</td></tr><tr><td>best_epoch</td><td>0</td></tr><tr><td>val_word_acc</td><td>0.0001</td></tr><tr><td>train_word_acc</td><td>0.0002</td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run history:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>epoch</td><td>▁</td></tr><tr><td>loss</td><td>▁</td></tr><tr><td>accuracy</td><td>▁</td></tr><tr><td>val_loss</td><td>▁</td></tr><tr><td>val_accuracy</td><td>▁</td></tr><tr><td>_runtime</td><td>▁█</td></tr><tr><td>_timestamp</td><td>▁█</td></tr><tr><td>_step</td><td>▁█</td></tr><tr><td>val_word_acc</td><td>▁</td></tr><tr><td>train_word_acc</td><td>▁</td></tr></table><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    <br/>Synced <strong style=\"color:#cdcd00\">silvery-sweep-2</strong>: <a href=\"https://wandb.ai/sonagara/DL_A3_Q1_testing1/runs/gkkkojcj\" target=\"_blank\">https://wandb.ai/sonagara/DL_A3_Q1_testing1/runs/gkkkojcj</a><br/>\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: c5yx5vew with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_layers: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_layers: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel: LSTM\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: rmsprop\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.30<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">crimson-sweep-3</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/sonagara/DL_A3_Q1_testing1\" target=\"_blank\">https://wandb.ai/sonagara/DL_A3_Q1_testing1</a><br/>\n",
              "                Sweep page: <a href=\"https://wandb.ai/sonagara/DL_A3_Q1_testing1/sweeps/u6pg8nh7\" target=\"_blank\">https://wandb.ai/sonagara/DL_A3_Q1_testing1/sweeps/u6pg8nh7</a><br/>\n",
              "Run page: <a href=\"https://wandb.ai/sonagara/DL_A3_Q1_testing1/runs/c5yx5vew\" target=\"_blank\">https://wandb.ai/sonagara/DL_A3_Q1_testing1/runs/c5yx5vew</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210517_114957-c5yx5vew</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "model_LSTM_o_rmsprop_el_2_dl_1_hs_128_em_128_d_0.1\n",
            "208/823 [======>.......................] - ETA: 22s - loss: 1.4987 - accuracy: 0.6681"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLk-ZBf1p4Ep"
      },
      "source": [
        "# best model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeqMWOAzqCXX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "901b55f1-a5ab-4859-8955-e30f31b5fd9f"
      },
      "source": [
        "def train_best():\n",
        "  m_name = \"GRU\"\n",
        "  encoder_layers = 3\n",
        "  decoder_layers = 3\n",
        "  latent_dim = 128\n",
        "  embedding_size = 128\n",
        "  dropout = 0.1\n",
        "  batch_size = 128\n",
        "  recurrent_dropout = 0  # 0 to use cudnnlstm which is faster than lstm\n",
        "  optimizer = \"adam\"\n",
        "  encoder_input_data,decoder_input_data ,decoder_target_data = load_and_preprocess()\n",
        "  val_encoder_input_data,val_decoder_input_data ,val_decoder_target_data = load_val_data()\n",
        "  \n",
        "\n",
        "  model = create_model(m_name,encoder_layers,decoder_layers,latent_dim,embedding_size,dropout,recurrent_dropout)\n",
        "  # model = keras.models.load_model(\"s2s\")\n",
        "  model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "  es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
        "\n",
        "  model.fit(\n",
        "    [encoder_input_data, decoder_input_data],\n",
        "    decoder_target_data,\n",
        "    batch_size=batch_size,\n",
        "    epochs=20,\n",
        "    # validation_split = 0.2,\n",
        "    validation_data=([val_encoder_input_data, val_decoder_input_data],val_decoder_target_data),\n",
        "    callbacks=[es]\n",
        "  )\n",
        "\n",
        "  temp = model.predict([val_encoder_input_data, val_decoder_input_data]).argmax(axis=-1)\n",
        "  val_word_acc = sum((temp[:,:-1] == val_decoder_input_data[:,1:]).all(axis=-1)) / len(val_encoder_input_data)\n",
        "  print(val_word_acc)\n",
        "\n",
        "  temp = model.predict([encoder_input_data,decoder_input_data]).argmax(axis=-1)\n",
        "  train_word_acc = sum((temp[:,:-1] == decoder_input_data[:,1:]).all(axis=-1)) / len(encoder_input_data)\n",
        "  print(train_word_acc)\n",
        "  \n",
        "\n",
        "\n",
        "  # Save model\n",
        "  model.save(\"s2s\")\n",
        "  # print(test_acc(\"LSTM\",100,1,2))\n",
        "train_best()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "823/823 [==============================] - 57s 23ms/step - loss: 1.3186 - accuracy: 0.6925 - val_loss: 0.8526 - val_accuracy: 0.7584\n",
            "Epoch 2/20\n",
            "823/823 [==============================] - 17s 21ms/step - loss: 0.8142 - accuracy: 0.7657 - val_loss: 0.6200 - val_accuracy: 0.8201\n",
            "Epoch 3/20\n",
            "823/823 [==============================] - 17s 20ms/step - loss: 0.5997 - accuracy: 0.8219 - val_loss: 0.4447 - val_accuracy: 0.8639\n",
            "Epoch 4/20\n",
            "823/823 [==============================] - 17s 20ms/step - loss: 0.4130 - accuracy: 0.8736 - val_loss: 0.3071 - val_accuracy: 0.9040\n",
            "Epoch 5/20\n",
            "823/823 [==============================] - 17s 21ms/step - loss: 0.2856 - accuracy: 0.9109 - val_loss: 0.2438 - val_accuracy: 0.9237\n",
            "Epoch 6/20\n",
            "823/823 [==============================] - 17s 21ms/step - loss: 0.2178 - accuracy: 0.9314 - val_loss: 0.2115 - val_accuracy: 0.9335\n",
            "Epoch 7/20\n",
            "823/823 [==============================] - 17s 21ms/step - loss: 0.1795 - accuracy: 0.9433 - val_loss: 0.1972 - val_accuracy: 0.9379\n",
            "Epoch 8/20\n",
            "823/823 [==============================] - 17s 21ms/step - loss: 0.1533 - accuracy: 0.9515 - val_loss: 0.1897 - val_accuracy: 0.9415\n",
            "Epoch 9/20\n",
            "823/823 [==============================] - 17s 21ms/step - loss: 0.1345 - accuracy: 0.9572 - val_loss: 0.1844 - val_accuracy: 0.9426\n",
            "Epoch 10/20\n",
            "823/823 [==============================] - 17s 21ms/step - loss: 0.1205 - accuracy: 0.9616 - val_loss: 0.1834 - val_accuracy: 0.9436\n",
            "Epoch 11/20\n",
            "823/823 [==============================] - 17s 21ms/step - loss: 0.1093 - accuracy: 0.9650 - val_loss: 0.1826 - val_accuracy: 0.9440\n",
            "Epoch 12/20\n",
            "823/823 [==============================] - 17s 21ms/step - loss: 0.1001 - accuracy: 0.9681 - val_loss: 0.1849 - val_accuracy: 0.9437\n",
            "Epoch 13/20\n",
            "823/823 [==============================] - 17s 21ms/step - loss: 0.0924 - accuracy: 0.9703 - val_loss: 0.1875 - val_accuracy: 0.9438\n",
            "Epoch 14/20\n",
            "823/823 [==============================] - 17s 21ms/step - loss: 0.0858 - accuracy: 0.9724 - val_loss: 0.1859 - val_accuracy: 0.9446\n",
            "Epoch 00014: early stopping\n",
            "0.3299750527729802\n",
            "0.7024138618940409\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_fn, gru_cell_1_layer_call_and_return_conditional_losses, gru_cell_1_layer_call_fn, gru_cell_2_layer_call_and_return_conditional_losses while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_fn, gru_cell_1_layer_call_and_return_conditional_losses, gru_cell_1_layer_call_fn, gru_cell_2_layer_call_and_return_conditional_losses while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: s2s/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: s2s/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsdsyaOXFW8J"
      },
      "source": [
        "import re\n",
        "def load_test_data():\n",
        "  global input_token_index , MAX_LEN_input , num_encoder_tokens , input_tokenizer\n",
        "  data_path = 'dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.test.tsv'\n",
        "  input_texts=[]\n",
        "  target_texts = []\n",
        "  s = r'ૠ'\n",
        "  # input_characters = set()\n",
        "  with open(data_path,\"r\",encoding=\"utf-8\") as f:\n",
        "    lines = f.read().split(\"\\n\")\n",
        "  for line in lines[:-1]:\n",
        "    temp = line.split(\"\\t\")\n",
        "    input_text , target_text = temp[1] ,temp[0]\n",
        "    target_text = re.sub(s,'ઋ',target_text)\n",
        "    input_text = input_text + \"\\n\"\n",
        "    target_text = target_text + \"\\n\"\n",
        "    \n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "  \n",
        "  encoder_input = input_tokenizer.texts_to_sequences(input_texts)\n",
        "  encoder_input_data = pad_sequences(encoder_input, maxlen=MAX_LEN_input, dtype='int32', padding='post', truncating='post',value= input_token_index[\"\\n\"])\n",
        "  print(encoder_input_data.shape)\n",
        "  \n",
        "  return encoder_input_data , input_texts ,target_texts\n",
        "# load_test_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koZFQ2Ns1k-R"
      },
      "source": [
        "def enc_dec(m_name=\"LSTM\" ,latent_dim = 100, n_e_layers = 1,n_d_layers = 1):\n",
        "  # keras.backend.clear_session()\n",
        "  model = keras.models.load_model(\"s2s\")\n",
        "  # model.summary()\n",
        " \n",
        "  if (n_e_layers == 1):\n",
        "    l_name = \"\"\n",
        "  else:\n",
        "    l_name = \"_\"+str(n_e_layers-1)\n",
        "\n",
        "  if (m_name == \"SimpleRNN\"):\n",
        "    n_name = \"simple_rnn\"\n",
        "  else:\n",
        "    n_name = m_name\n",
        "\n",
        "  # encoder\n",
        "  encoder_inputs = model.input[0]\n",
        "  encoder_outputs, *encoder_states = model.get_layer(n_name.lower()+l_name).output  # last encoding layer\n",
        "  encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
        "  # encoder_model.summary()\n",
        "\n",
        "  # decoder\n",
        "  decoder_inputs = model.input[1] \n",
        "  decoder_embed = model.get_layer(\"embedding_1\")(decoder_inputs)\n",
        "  \n",
        "  decoder_states_inputs = []\n",
        "  decoder_states_inputs.append(keras.Input(shape=(latent_dim,)))\n",
        "  if (m_name == \"LSTM\"):\n",
        "    decoder_states_inputs.append(keras.Input(shape=(latent_dim,)))\n",
        "  \n",
        "  d_o = model.get_layer(n_name.lower()+\"_\"+str(n_e_layers))(decoder_embed, initial_state=decoder_states_inputs)\n",
        "  for i in range(1+n_e_layers,n_d_layers+n_e_layers):\n",
        "    d_o = model.get_layer(n_name.lower()+\"_\"+str(i))(d_o[0], initial_state=decoder_states_inputs)\n",
        "  decoder_outputs = d_o[0]\n",
        "  decoder_states = d_o[1:]\n",
        "  decoder_dense = model.get_layer(\"dense\")\n",
        "  decoder_outputs = decoder_dense(decoder_outputs)\n",
        "  decoder_model = keras.Model([decoder_inputs ,  decoder_states_inputs], [decoder_outputs] + decoder_states)\n",
        "  # decoder_model.summary()\n",
        "\n",
        "  return encoder_model , decoder_model\n",
        "# enc_dec(\"LSTM\",256,3,2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Us-kQ5OtmON9"
      },
      "source": [
        "# Reverse-lookup token index to decode sequences back to\n",
        "# something readable.\n",
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
        "\n",
        "\n",
        "def decode_sequence(input_seq,encoder_model,decoder_model):\n",
        "    global num_decoder_tokens , target_token_index , reverse_target_char_index , MAX_LEN_target\n",
        "    # Encode the input as state vectors.\n",
        "\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    # print(states_value)\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0] = target_token_index[\"\\t\"]\n",
        "    # print(target_seq)\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = \"\"\n",
        "    while not stop_condition:\n",
        "        output_tokens, *states = decoder_model.predict([target_seq, states_value])\n",
        "        # print(len(states))\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if sampled_char == \"\\n\" or len(decoded_sentence) > MAX_LEN_target:\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = target_token_index[sampled_char]\n",
        "\n",
        "        # Update states\n",
        "        states_value = states\n",
        "    return decoded_sentence\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVVdX2PjPULK"
      },
      "source": [
        "decode_results = []\n",
        "test_input = None\n",
        "def test_acc(m_name=\"LSTM\" ,latent_dim = 100, n_e_layers = 1,n_d_layers = 1):\n",
        "  global decode_results,test_input\n",
        "  x_test ,x ,y_test = load_test_data()\n",
        "  test_input = x\n",
        "  enc ,dec = enc_dec(m_name , latent_dim , n_e_layers ,n_d_layers)\n",
        "  score = 0 \n",
        "  # print(len(y_test))\n",
        "\n",
        "  for seq_index in range(len(y_test)):\n",
        "    \n",
        "    input_seq = x_test[seq_index : seq_index + 1]\n",
        "    print(x[seq_index])\n",
        "    print(input_seq)\n",
        "    decoded_sentence = decode_sequence(input_seq,enc,dec)\n",
        "    decode_results.append(decoded_sentence)\n",
        "    if (y_test[seq_index] == decoded_sentence):\n",
        "      score += 1\n",
        "    # print(\"-\")\n",
        "    # print(\"original sentence:\", y_test[seq_index])\n",
        "    # print(\"Decoded sentence:\", decoded_sentence)\n",
        "  print(score/len(y_test))\n",
        "  return score/len(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jck7wH6TqrXW"
      },
      "source": [
        "print(test_acc(\"GRU\",128,3,3)) #0.32623156271088405"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-GDVcKCgTIy"
      },
      "source": [
        "def save_csv():\n",
        "  global test_input , decode_results\n",
        "  dict = {\"Input Sentence\" : test_input , \"Decoded Sentence\" : decode_results}\n",
        "  df = pd.DataFrame(dict)\n",
        "  df.to_csv(\"predictions_vanilla.csv\")\n",
        "save_csv()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XijCJhHEWPBK",
        "outputId": "406997a6-db0f-4067-ec48-09b45d924333"
      },
      "source": [
        "def grid():\n",
        "  global test_input,decode_results\n",
        "  _,_,y = load_test_data()\n",
        "  for i in range(30,40):\n",
        "    if (y[i] == decode_results[i]):\n",
        "      print(\"\\033[1;32;47m\"+test_input[i] +\"\\t\"+ \"\\033[1;32;47m\"+ decode_results[i])\n",
        "    else:\n",
        "      print(\"\\033[1;31;47m\" +test_input[i]+ \"\\t\"+\"\\033[1;31;47m\"+ decode_results[i])\n",
        "\n",
        "grid()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10373, 23)\n",
            "\u001b[1;31;47mangrejimaan\n",
            "\t\u001b[1;31;47mઅંગ્રેજોમાં\n",
            "\n",
            "\u001b[1;31;47mangrejiman\n",
            "\t\u001b[1;31;47mઅંગ્રેજોમાં\n",
            "\n",
            "\u001b[1;31;47manjaaramaa\n",
            "\t\u001b[1;31;47mઅંજરરાાં\n",
            "\n",
            "\u001b[1;31;47manjaarma\n",
            "\t\u001b[1;31;47mઅંજરરમાં\n",
            "\n",
            "\u001b[1;32;47manjaarmaan\n",
            "\t\u001b[1;32;47mઅંજારમાં\n",
            "\n",
            "\u001b[1;31;47manjaarman\n",
            "\t\u001b[1;31;47mઅંજરરમાં\n",
            "\n",
            "\u001b[1;31;47manjarma\n",
            "\t\u001b[1;31;47mઅંજરરમાં\n",
            "\n",
            "\u001b[1;31;47manjarmaan\n",
            "\t\u001b[1;31;47mઅંજરરમાં\n",
            "\n",
            "\u001b[1;31;47manjarman\n",
            "\t\u001b[1;31;47mઅંજરરમાં\n",
            "\n",
            "\u001b[1;32;47mandhaapo\n",
            "\t\u001b[1;32;47mઅંધાપો\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_acIJjs-QhTo",
        "outputId": "64515247-f8d0-41d6-94a6-a3f48a539350"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0VzUWkeRg3_",
        "outputId": "7f057009-c5a6-4d36-c843-73e7e4b0fc35"
      },
      "source": [
        "model = keras.models.load_model(\"s2s\")\n",
        "model.save(\"drive/My Drive/s2s\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_24_layer_call_and_return_conditional_losses, gru_cell_24_layer_call_fn, gru_cell_25_layer_call_and_return_conditional_losses, gru_cell_25_layer_call_fn, gru_cell_26_layer_call_and_return_conditional_losses while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as gru_cell_24_layer_call_and_return_conditional_losses, gru_cell_24_layer_call_fn, gru_cell_25_layer_call_and_return_conditional_losses, gru_cell_25_layer_call_fn, gru_cell_26_layer_call_and_return_conditional_losses while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: drive/My Drive/s2s/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: drive/My Drive/s2s/assets\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}