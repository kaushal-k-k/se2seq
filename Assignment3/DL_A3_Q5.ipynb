{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_A3_Q5.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "uRGyFIT_axmv",
        "RfOXyhI9eGCB",
        "K8f-bZvnYh2q"
      ],
      "authorship_tag": "ABX9TyO+gaHP5Mdj0cGkc2PjzVkI"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRGyFIT_axmv"
      },
      "source": [
        "# download Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IntdRGraoSN",
        "outputId": "06d6cf0e-d378-4c62-c293-22080ec51407"
      },
      "source": [
        "!wget https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
        "!tar -xvf  'dakshina_dataset_v1.0.tar'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-26 03:47:07--  https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.97.128, 64.233.170.128, 74.125.31.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.97.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2008340480 (1.9G) [application/x-tar]\n",
            "Saving to: ‘dakshina_dataset_v1.0.tar’\n",
            "\n",
            "dakshina_dataset_v1 100%[===================>]   1.87G   124MB/s    in 17s     \n",
            "\n",
            "2021-05-26 03:47:24 (116 MB/s) - ‘dakshina_dataset_v1.0.tar’ saved [2008340480/2008340480]\n",
            "\n",
            "dakshina_dataset_v1.0/bn/\n",
            "dakshina_dataset_v1.0/bn/lexicons/\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/romanized/\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/gu/\n",
            "dakshina_dataset_v1.0/gu/lexicons/\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/romanized/\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/hi/\n",
            "dakshina_dataset_v1.0/hi/lexicons/\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/hi/romanized/\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/kn/\n",
            "dakshina_dataset_v1.0/kn/lexicons/\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/kn/romanized/\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ml/\n",
            "dakshina_dataset_v1.0/ml/lexicons/\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/tmp.rom.txt\n",
            "dakshina_dataset_v1.0/ml/lexicons/tmp.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/romanized/\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/mr/\n",
            "dakshina_dataset_v1.0/mr/lexicons/\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/romanized/\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/pa/\n",
            "dakshina_dataset_v1.0/pa/lexicons/\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/pa/romanized/\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/sd/\n",
            "dakshina_dataset_v1.0/sd/lexicons/\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/sd/romanized/\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/si/\n",
            "dakshina_dataset_v1.0/si/lexicons/\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/romanized/\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ta/\n",
            "dakshina_dataset_v1.0/ta/lexicons/\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/romanized/\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/te/\n",
            "dakshina_dataset_v1.0/te/lexicons/\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/romanized/\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ur/\n",
            "dakshina_dataset_v1.0/ur/lexicons/\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/romanized/\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/README.md\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlX_ycI-a3cD"
      },
      "source": [
        "# load data and process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jyB-MXYatP8",
        "outputId": "24b46fb9-e744-4c63-d068-c2f98494d82e"
      },
      "source": [
        "%pip install wandb -q\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.8MB 5.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 24.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 174kB 20.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 9.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 7.5MB/s \n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-9k_tSca8-P"
      },
      "source": [
        "input_token_index = None\n",
        "target_token_index = None\n",
        "MAX_LEN_input = None\n",
        "MAX_LEN_target = None\n",
        "num_encoder_tokens = 30\n",
        "num_decoder_tokens = 70\n",
        "input_tokenizer = None\n",
        "target_tokenizer = None"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4KPNp7GbBZW"
      },
      "source": [
        "def tokenize(data,vocab_size):\n",
        "  tokenizer = Tokenizer(num_words=vocab_size,char_level=True)\n",
        "  tokenizer.fit_on_texts(data)\n",
        "  temp=tokenizer.texts_to_sequences(data)\n",
        "  # print(data[:3])\n",
        "  # print(temp[:3])\n",
        "  dictionary = tokenizer.word_index\n",
        "  return temp , dictionary , tokenizer"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qacRMXz9bCPu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c30a3d8b-c640-4c83-ce76-2e1338d5f9fd"
      },
      "source": [
        "def load_and_preprocess():\n",
        "  global input_token_index , target_token_index , MAX_LEN_input , MAX_LEN_target ,num_decoder_tokens,num_encoder_tokens , input_tokenizer , target_tokenizer\n",
        "  data_path = 'dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.train.tsv'\n",
        "  input_texts = []\n",
        "  target_texts = []\n",
        "  with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.read().split(\"\\n\")\n",
        "  for line in lines[:-1]:\n",
        "    temp = line.split('\\t')\n",
        "    input_text, target_text = temp[1],temp[0] \n",
        "    target_text = \"\\t\" + target_text + \"\\n\"\n",
        "    input_text = input_text+\"\\n\"\n",
        "\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "  \n",
        "  MAX_LEN_input = max([len(txt) for txt in input_texts])\n",
        "  MAX_LEN_target = max([len(txt) for txt in target_texts])\n",
        "\n",
        "  # toeknize\n",
        "  encoder_input , input_token_index , input_tokenizer = tokenize(input_texts , num_encoder_tokens)\n",
        "  decoder_input , target_token_index, target_tokenizer = tokenize(target_texts , num_decoder_tokens) \n",
        "\n",
        "  # padding\n",
        "  encoder_input_data = pad_sequences(encoder_input, maxlen=MAX_LEN_input, dtype='int32', padding='post', truncating='post',value= input_token_index[\"\\n\"])\n",
        "  decoder_input_data = pad_sequences(decoder_input, maxlen=MAX_LEN_target, dtype='int32', padding='post', truncating='post',value=target_token_index[\"\\n\"])\n",
        "\n",
        "  decoder_target_data = np.zeros((len(input_texts), MAX_LEN_target, num_decoder_tokens), dtype=\"float32\")\n",
        "  for i,  target_text in enumerate(target_texts):\n",
        "    for t, char in enumerate(target_text):\n",
        "      if t > 0:\n",
        "        decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "    decoder_target_data[i, t:, target_token_index[\"\\n\"]] = 1.0\n",
        "\n",
        "  return encoder_input_data , decoder_input_data, decoder_target_data\n",
        "\n",
        "load_and_preprocess()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[ 1,  4, 12, ...,  2,  2,  2],\n",
              "        [ 1,  4, 12, ...,  2,  2,  2],\n",
              "        [ 1,  4, 12, ...,  2,  2,  2],\n",
              "        ...,\n",
              "        [ 5,  6,  1, ...,  2,  2,  2],\n",
              "        [ 1, 16, 11, ...,  2,  2,  2],\n",
              "        [ 9, 11,  2, ...,  2,  2,  2]], dtype=int32),\n",
              " array([[ 1, 38, 10, ...,  2,  2,  2],\n",
              "        [ 1, 38, 10, ...,  2,  2,  2],\n",
              "        [ 1, 38, 10, ...,  2,  2,  2],\n",
              "        ...,\n",
              "        [ 1, 28,  6, ...,  2,  2,  2],\n",
              "        [ 1, 65,  2, ...,  2,  2,  2],\n",
              "        [ 1, 65,  2, ...,  2,  2,  2]], dtype=int32),\n",
              " array([[[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 1., ..., 0., 0., 0.],\n",
              "         [0., 0., 1., ..., 0., 0., 0.],\n",
              "         [0., 0., 1., ..., 0., 0., 0.]],\n",
              " \n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 1., ..., 0., 0., 0.],\n",
              "         [0., 0., 1., ..., 0., 0., 0.],\n",
              "         [0., 0., 1., ..., 0., 0., 0.]],\n",
              " \n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 1., ..., 0., 0., 0.],\n",
              "         [0., 0., 1., ..., 0., 0., 0.],\n",
              "         [0., 0., 1., ..., 0., 0., 0.]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 1., ..., 0., 0., 0.],\n",
              "         [0., 0., 1., ..., 0., 0., 0.],\n",
              "         [0., 0., 1., ..., 0., 0., 0.]],\n",
              " \n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 1., ..., 0., 0., 0.],\n",
              "         [0., 0., 1., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 1., ..., 0., 0., 0.],\n",
              "         [0., 0., 1., ..., 0., 0., 0.],\n",
              "         [0., 0., 1., ..., 0., 0., 0.]],\n",
              " \n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 1., ..., 0., 0., 0.],\n",
              "         [0., 0., 1., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 1., ..., 0., 0., 0.],\n",
              "         [0., 0., 1., ..., 0., 0., 0.],\n",
              "         [0., 0., 1., ..., 0., 0., 0.]]], dtype=float32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNOUxbiydfvj"
      },
      "source": [
        "def load_val_data(data_path = 'dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.dev.tsv'):\n",
        "  global input_token_index , target_token_index , MAX_LEN_input , MAX_LEN_target ,num_decoder_tokens,num_encoder_tokens , input_tokenizer , target_tokenizer\n",
        "  \n",
        "  input_texts = []\n",
        "  target_texts = []\n",
        "  with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.read().split(\"\\n\")\n",
        "  for line in lines[:-1]:\n",
        "    temp = line.split('\\t')\n",
        "    input_text, target_text = temp[1],temp[0] \n",
        "    target_text = \"\\t\" + target_text + \"\\n\"\n",
        "    input_text = input_text+\"\\n\"\n",
        "\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "  \n",
        "  # tokenize\n",
        "  encoder_input  = input_tokenizer.texts_to_sequences(input_texts)\n",
        "  decoder_input  = target_tokenizer.texts_to_sequences(target_texts) \n",
        "\n",
        "  # padding\n",
        "  encoder_input_data = pad_sequences(encoder_input, maxlen=MAX_LEN_input, dtype='int32', padding='post', truncating='post',value= input_token_index[\"\\n\"])\n",
        "  decoder_input_data = pad_sequences(decoder_input, maxlen=MAX_LEN_target, dtype='int32', padding='post', truncating='post',value=target_token_index[\"\\n\"])\n",
        "\n",
        "  decoder_target_data = np.zeros((len(input_texts), MAX_LEN_target, num_decoder_tokens), dtype=\"float32\")\n",
        "  for i,  target_text in enumerate(target_texts):\n",
        "    for t, char in enumerate(target_text):\n",
        "      if char == 'ૠ':\n",
        "        char = 'ઋ'\n",
        "      if t > 0:\n",
        "        decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "    decoder_target_data[i, t:, target_token_index[\"\\n\"]] = 1.0\n",
        "\n",
        "  return encoder_input_data , decoder_input_data, decoder_target_data\n",
        "\n",
        "# load_val_data()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e3vrfyUcDN_"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow.python.keras.layers import Layer, Concatenate\n",
        "from tensorflow.python.keras import backend as K\n",
        "\n",
        "\n",
        "class AttentionLayer(Layer):\n",
        "    \"\"\"\n",
        "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
        "    There are three sets of weights introduced W_a, U_a, and V_a\n",
        "     \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert isinstance(input_shape, list)\n",
        "        # Create a trainable weight variable for this layer.\n",
        "\n",
        "        self.W_a = self.add_weight(name='W_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.U_a = self.add_weight(name='U_a',\n",
        "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.V_a = self.add_weight(name='V_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "\n",
        "        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "    def call(self, inputs, verbose=False):\n",
        "        \"\"\"\n",
        "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
        "        \"\"\"\n",
        "        assert type(inputs) == list\n",
        "        encoder_out_seq, decoder_out_seq = inputs\n",
        "        if verbose:\n",
        "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
        "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
        "\n",
        "        def energy_step(inputs, states):\n",
        "            \"\"\" Step function for computing energy for a single decoder state\n",
        "            inputs: (batchsize * 1 * de_in_dim)\n",
        "            states: (batchsize * 1 * de_latent_dim)\n",
        "            \"\"\"\n",
        "\n",
        "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n",
        "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
        "\n",
        "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
        "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
        "            de_hidden = inputs.shape[-1]\n",
        "\n",
        "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
        "            # <= batch size * en_seq_len * latent_dim\n",
        "            W_a_dot_s = K.dot(encoder_out_seq, self.W_a)\n",
        "\n",
        "            \"\"\" Computing hj.Ua \"\"\"\n",
        "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
        "            if verbose:\n",
        "                print('Ua.h>', U_a_dot_h.shape)\n",
        "\n",
        "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            Ws_plus_Uh = K.tanh(W_a_dot_s + U_a_dot_h)\n",
        "            if verbose:\n",
        "                print('Ws+Uh>', Ws_plus_Uh.shape)\n",
        "\n",
        "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.squeeze(K.dot(Ws_plus_Uh, self.V_a), axis=-1)\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.softmax(e_i)\n",
        "\n",
        "            if verbose:\n",
        "                print('ei>', e_i.shape)\n",
        "\n",
        "            return e_i, [e_i]\n",
        "\n",
        "        def context_step(inputs, states):\n",
        "            \"\"\" Step function for computing ci using ei \"\"\"\n",
        "\n",
        "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n",
        "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
        "\n",
        "            # <= batch_size, hidden_size\n",
        "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
        "            if verbose:\n",
        "                print('ci>', c_i.shape)\n",
        "            return c_i, [c_i]\n",
        "\n",
        "        fake_state_c = K.sum(encoder_out_seq, axis=1)\n",
        "        fake_state_e = K.sum(encoder_out_seq, axis=2)  # <= (batch_size, enc_seq_len, latent_dim\n",
        "\n",
        "        \"\"\" Computing energy outputs \"\"\"\n",
        "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
        "        last_out, e_outputs, _ = K.rnn(\n",
        "            energy_step, decoder_out_seq, [fake_state_e],\n",
        "        )\n",
        "\n",
        "        \"\"\" Computing context vectors \"\"\"\n",
        "        last_out, c_outputs, _ = K.rnn(\n",
        "            context_step, e_outputs, [fake_state_c],\n",
        "        )\n",
        "\n",
        "        return c_outputs, e_outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \"\"\" Outputs produced by the layer \"\"\"\n",
        "        return [\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
        "        ]\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sd0yQw9ubKFK"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense,LSTM,Input,GRU,SimpleRNN,Dropout,Embedding\n",
        "\n",
        "def create_model_attention(m_name=\"LSTM\",n_e_layers=1,n_d_layers=1,latent_dim = 100,embedding_size = 16,dropout = 0 , recurrent_dropout = 0):\n",
        "  global num_decoder_tokens,num_decoder_tokens\n",
        "  keras.backend.clear_session()\n",
        "  \n",
        "  # Define an input sequence and process it.\n",
        "  input1 = Input(shape=(None,),name= \"input_1\")\n",
        "  encoder_inputs = Embedding(input_dim = num_encoder_tokens, output_dim = embedding_size)(input1)\n",
        "\n",
        "  encoder = globals()[m_name](latent_dim,dropout=dropout,recurrent_dropout = recurrent_dropout, return_state=True,return_sequences=True)\n",
        "  e_o = encoder(encoder_inputs)\n",
        "  prev = e_o\n",
        "  for i in range(1,n_e_layers):\n",
        "    e = globals()[m_name](latent_dim, dropout=dropout,recurrent_dropout = recurrent_dropout,return_state=True,return_sequences=True)\n",
        "    e_o = e(prev[0])\n",
        "    prev = e_o\n",
        "  \n",
        "  input2 = Input(shape=(None,),name=\"input_2\")\n",
        "  decoder_inputs = Embedding(input_dim = num_decoder_tokens, output_dim = embedding_size)(input2)\n",
        "  d_l = globals()[m_name](latent_dim,dropout=dropout,recurrent_dropout = recurrent_dropout, return_sequences=True, return_state=True)(decoder_inputs,initial_state = e_o[1:])\n",
        "  p_d = d_l[0]\n",
        "  for i in range(1,n_d_layers):\n",
        "    d_l = globals()[m_name](latent_dim,dropout=dropout,recurrent_dropout = recurrent_dropout, return_state=True, return_sequences=True)(p_d,initial_state = e_o[1:])\n",
        "    p_d = d_l[0]\n",
        "\n",
        "  attn_layer = AttentionLayer(name=\"attention_layer\")\n",
        "  attn_op, attn_state = attn_layer([e_o[0], d_l[0]])\n",
        "  decoder_concat_input = Concatenate(axis=-1)([d_l[0], attn_op])\n",
        "\n",
        "\n",
        "  dec_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "  final_output = dec_dense(decoder_concat_input)\n",
        " \n",
        "\n",
        "  # Define the model that will turn\n",
        "  # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "  model = keras.Model([input1,input2], final_output)\n",
        "\n",
        "  return model\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfOXyhI9eGCB"
      },
      "source": [
        "# sweep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M21Ip4O-bNDb"
      },
      "source": [
        "def train_attention():\n",
        "  run = wandb.init()\n",
        "  c = run.config\n",
        "  name = \"model_\"+c.model+\"_o_\"+c.optimizer+\"_hs_\"+str(c.hidden_size)+\"_em_\"+str(c.embedding_size)+\"_d_\"+str(c.dropout)+\"_bs_\"+str(c.batch_size)\n",
        "  run.name = name\n",
        "  print(name)\n",
        "  batch_size = c.batch_size\n",
        "  epochs = 20\n",
        "\n",
        "  # used single encoder and decoder layer\n",
        "  encoder_layers , decoder_layers = 1 , 1\n",
        "\n",
        "\n",
        "  encoder_input_data,decoder_input_data ,decoder_target_data = load_and_preprocess()\n",
        "  val_encoder_input_data,val_decoder_input_data ,val_decoder_target_data = load_val_data()\n",
        "\n",
        "  es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
        "\n",
        "  model = create_model_attention(c.model,encoder_layers,decoder_layers,c.hidden_size,c.embedding_size,c.dropout,0)\n",
        "  model.compile(optimizer=c.optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "  model.fit(\n",
        "    [encoder_input_data, decoder_input_data],\n",
        "    decoder_target_data,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=([val_encoder_input_data, val_decoder_input_data],val_decoder_target_data),\n",
        "    # validation_split=0.2,\n",
        "    callbacks=[WandbCallback(),es]\n",
        "  )\n",
        "  \n",
        "  temp = model.predict([val_encoder_input_data, val_decoder_input_data]).argmax(axis=-1)\n",
        "  val_word_acc = sum((temp[:,:-1] == val_decoder_input_data[:,1:]).all(axis=-1)) / len(val_encoder_input_data)\n",
        "  # print(val_word_acc)\n",
        "\n",
        "  temp = model.predict([encoder_input_data,decoder_input_data]).argmax(axis=-1)\n",
        "  train_word_acc = sum((temp[:,:-1] == decoder_input_data[:,1:]).all(axis=-1)) / len(encoder_input_data)\n",
        "  # print(train_word_acc)\n",
        "\n",
        "  wandb.log({\"val_word_acc\" : round(val_word_acc,4) , \"train_word_acc\" : round(train_word_acc,4)})\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBLtbSTMeV4W"
      },
      "source": [
        "sweep_config_attention={\n",
        "    'method' : 'random' ,\n",
        "    'metric' : { 'name' : 'val_word_acc' , 'goal' : 'maximize' } ,\n",
        "    'parameters' : {\n",
        "        'model' : { 'values' : ['LSTM','GRU','SimpleRNN'] },\n",
        "        'dropout' : { 'values' : [0.1,0.2,0.3]},\n",
        "        'embedding_size' : {'values' : [32,64,128]},\n",
        "        'hidden_size' : {'values' : [128,256,512]},\n",
        "        'batch_size' : {'values' : [64,128]},\n",
        "        'optimizer' : {'values' : ['rmsprop' ,'adam']}\n",
        "    }\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "MXcJYQLriFNT",
        "outputId": "97cd108c-f636-44b3-8822-0cb77c396872"
      },
      "source": [
        "\n",
        "sweepid = wandb.sweep(sweep_config_attention,project=\"DL_A3_Q5_final\",entity =\"sonagara\")\n",
        "wandb.agent(sweepid,train_attention)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Create sweep with ID: 8azc26kw\n",
            "Sweep URL: https://wandb.ai/sonagara/DL_A3_Q5_testing1/sweeps/8azc26kw\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: bo9mqqa0 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 512\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel: SimpleRNN\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.30<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">worldly-sweep-1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/sonagara/DL_A3_Q5_testing1\" target=\"_blank\">https://wandb.ai/sonagara/DL_A3_Q5_testing1</a><br/>\n",
              "                Sweep page: <a href=\"https://wandb.ai/sonagara/DL_A3_Q5_testing1/sweeps/8azc26kw\" target=\"_blank\">https://wandb.ai/sonagara/DL_A3_Q5_testing1/sweeps/8azc26kw</a><br/>\n",
              "Run page: <a href=\"https://wandb.ai/sonagara/DL_A3_Q5_testing1/runs/bo9mqqa0\" target=\"_blank\">https://wandb.ai/sonagara/DL_A3_Q5_testing1/runs/bo9mqqa0</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210517_121756-bo9mqqa0</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "model_SimpleRNN_o_adam_hs_512_em_128_d_0.2_bs_128\n",
            "  2/823 [..............................] - ETA: 34:05 - loss: 4.0117 - accuracy: 0.1678  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAp20x7IeUBF"
      },
      "source": [
        "#  best model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8f-bZvnYh2q"
      },
      "source": [
        "## train best model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWjs9FCneYkb",
        "outputId": "5a43f841-3bd7-4517-a518-a4f81b0bc25e"
      },
      "source": [
        "def train_best():\n",
        "  m_name = \"LSTM\"\n",
        "  encoder_layers = 1\n",
        "  decoder_layers = 1\n",
        "  latent_dim = 128\n",
        "  embedding_size = 32\n",
        "  dropout = 0.3\n",
        "  batch_size = 64\n",
        "  recurrent_dropout = 0  # 0 to use cudnnlstm which is faster than lstm\n",
        "  optimizer = \"adam\"\n",
        "\n",
        "  encoder_input_data,decoder_input_data ,decoder_target_data = load_and_preprocess()\n",
        "  val_encoder_input_data,val_decoder_input_data ,val_decoder_target_data = load_val_data()\n",
        "  \n",
        "\n",
        "  model = create_model_attention(m_name,encoder_layers,decoder_layers,latent_dim,embedding_size,dropout,recurrent_dropout)\n",
        "  # model = keras.models.load_model(\"s2s\")\n",
        "  model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "  es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
        "\n",
        "  model.fit(\n",
        "    [encoder_input_data, decoder_input_data],\n",
        "    decoder_target_data,\n",
        "    batch_size=batch_size,\n",
        "    epochs=20,\n",
        "    # validation_split = 0.2,\n",
        "    validation_data=([val_encoder_input_data, val_decoder_input_data],val_decoder_target_data),\n",
        "    callbacks=[es]\n",
        "  )\n",
        "\n",
        "  temp = model.predict([val_encoder_input_data, val_decoder_input_data]).argmax(axis=-1)\n",
        "  val_word_acc = sum((temp[:,:-1] == val_decoder_input_data[:,1:]).all(axis=-1)) / len(val_encoder_input_data)\n",
        "  print(val_word_acc)\n",
        "\n",
        "  temp = model.predict([encoder_input_data,decoder_input_data]).argmax(axis=-1)\n",
        "  train_word_acc = sum((temp[:,:-1] == decoder_input_data[:,1:]).all(axis=-1)) / len(encoder_input_data)\n",
        "  print(train_word_acc)\n",
        "  \n",
        "\n",
        "\n",
        "  # Save model\n",
        "  model.save(\"s2sa\")\n",
        "  # print(test_acc(\"LSTM\",100,1,2))\n",
        "train_best()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1645/1645 [==============================] - 311s 186ms/step - loss: 1.2458 - accuracy: 0.7059 - val_loss: 0.7247 - val_accuracy: 0.7950\n",
            "Epoch 2/20\n",
            "1645/1645 [==============================] - 307s 187ms/step - loss: 0.5707 - accuracy: 0.8341 - val_loss: 0.2032 - val_accuracy: 0.9323\n",
            "Epoch 3/20\n",
            "1645/1645 [==============================] - 304s 185ms/step - loss: 0.2056 - accuracy: 0.9327 - val_loss: 0.1692 - val_accuracy: 0.9423\n",
            "Epoch 4/20\n",
            "1645/1645 [==============================] - 309s 188ms/step - loss: 0.1640 - accuracy: 0.9451 - val_loss: 0.1544 - val_accuracy: 0.9464\n",
            "Epoch 5/20\n",
            "1645/1645 [==============================] - 306s 186ms/step - loss: 0.1425 - accuracy: 0.9516 - val_loss: 0.1470 - val_accuracy: 0.9504\n",
            "Epoch 6/20\n",
            "1645/1645 [==============================] - 312s 190ms/step - loss: 0.1306 - accuracy: 0.9559 - val_loss: 0.1407 - val_accuracy: 0.9525\n",
            "Epoch 7/20\n",
            "1645/1645 [==============================] - 316s 192ms/step - loss: 0.1183 - accuracy: 0.9601 - val_loss: 0.1392 - val_accuracy: 0.9526\n",
            "Epoch 8/20\n",
            "1645/1645 [==============================] - 322s 196ms/step - loss: 0.1110 - accuracy: 0.9627 - val_loss: 0.1363 - val_accuracy: 0.9539\n",
            "Epoch 9/20\n",
            "1645/1645 [==============================] - 313s 190ms/step - loss: 0.1032 - accuracy: 0.9653 - val_loss: 0.1376 - val_accuracy: 0.9538\n",
            "Epoch 10/20\n",
            "1645/1645 [==============================] - 314s 191ms/step - loss: 0.0968 - accuracy: 0.9677 - val_loss: 0.1385 - val_accuracy: 0.9539\n",
            "Epoch 11/20\n",
            "1645/1645 [==============================] - 326s 198ms/step - loss: 0.0913 - accuracy: 0.9695 - val_loss: 0.1400 - val_accuracy: 0.9544\n",
            "Epoch 00011: early stopping\n",
            "0.3944540395317597\n",
            "0.6069613459108743\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: s2sa/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: s2sa/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jhNa16AYok-"
      },
      "source": [
        "## run inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sjoHxuUynu1"
      },
      "source": [
        "import re\n",
        "def load_test_data():\n",
        "  global input_token_index , MAX_LEN_input , num_encoder_tokens , input_tokenizer\n",
        "  data_path = 'dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.test.tsv'\n",
        "  input_texts=[]\n",
        "  target_texts = []\n",
        "  s = r'ૠ'\n",
        "  # input_characters = set()\n",
        "  with open(data_path,\"r\",encoding=\"utf-8\") as f:\n",
        "    lines = f.read().split(\"\\n\")\n",
        "  for line in lines[:-1]:\n",
        "    temp = line.split(\"\\t\")\n",
        "    input_text , target_text = temp[1] ,temp[0]\n",
        "    target_text = re.sub(s,'ઋ',target_text)\n",
        "    input_text = input_text + \"\\n\"\n",
        "    target_text = target_text + \"\\n\"\n",
        "    \n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "  \n",
        "  encoder_input = input_tokenizer.texts_to_sequences(input_texts)\n",
        "  encoder_input_data = pad_sequences(encoder_input, maxlen=MAX_LEN_input, dtype='int32', padding='post', truncating='post',value= input_token_index[\"\\n\"])\n",
        "  print(encoder_input_data.shape)\n",
        "  \n",
        "  return encoder_input_data ,input_texts, target_texts\n",
        "# load_test_data()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRCAPV3yHXZo"
      },
      "source": [
        "def enc_dec_attention(m_name=\"LSTM\",n_e_layers=1,n_d_layers=1,latent_dim = 100):\n",
        "  model = keras.models.load_model(\"s2sa\")\n",
        " \n",
        "  if (n_e_layers == 1):\n",
        "    l_name = \"\"\n",
        "  else:\n",
        "    l_name = \"_\"+str(n_e_layers-1)\n",
        "\n",
        "  if (m_name == \"SimpleRNN\"):\n",
        "    n_name = \"simple_rnn\"\n",
        "  else:\n",
        "    n_name = m_name\n",
        "  # model.summary()\n",
        "\n",
        "  # encoder\n",
        "  encoder_inputs = model.input[0]\n",
        "  encoder_outputs, *encoder_states = model.get_layer(n_name.lower()+l_name).output  # last encoding layer\n",
        "  encoder_model = tf.keras.models.Model(encoder_inputs, [encoder_outputs,encoder_states])\n",
        "  # encoder_model.summary()\n",
        "    \n",
        "\n",
        "  # decoder\n",
        "  decoder_inputs = model.input[1]\n",
        "  decoder_embed = model.get_layer(\"embedding_1\")(decoder_inputs)\n",
        "  decoder_states_inputs = []\n",
        "  decoder_states_inputs.append(keras.Input(shape=(latent_dim,)))\n",
        "  if (m_name == \"LSTM\"):\n",
        "    decoder_states_inputs.append(keras.Input(shape=(latent_dim,)))\n",
        "  decoder_outputs, *decoder_states = model.get_layer(n_name.lower()+\"_\"+str(n_e_layers))(decoder_embed, initial_state=decoder_states_inputs)\n",
        "  decoder_model = tf.keras.models.Model([decoder_inputs, decoder_states_inputs],[decoder_outputs] + decoder_states)\n",
        "  # decoder_model.summary()\n",
        "\n",
        "  atten_layer = model.get_layer(\"attention_layer\")\n",
        "  dense_layer = model.get_layer(\"dense\")\n",
        "\n",
        "  return encoder_model , decoder_model ,atten_layer ,dense_layer \n",
        "\n",
        "# enc_dec_attention()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHlT86NlOfRF"
      },
      "source": [
        "\n",
        "# Reverse-lookup token index to decode sequences back to\n",
        "# something readable.\n",
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
        "\n",
        "\n",
        "def decode_sequence_attention(input_seq,encoder_model,decoder_model,decoder_dense,atten_layer):\n",
        "    global num_decoder_tokens , target_token_index , reverse_target_char_index , MAX_LEN_target\n",
        "    # Encode the input as state vectors.\n",
        "\n",
        "    enc_op, states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0] = target_token_index[\"\\t\"]\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = \"\"\n",
        "    attn_state_arr = []\n",
        "    while not stop_condition:\n",
        "        dec_outputs, h,c = decoder_model.predict([target_seq , states_value])\n",
        "\n",
        "        atten_op, attn_state = atten_layer([enc_op, dec_outputs])\n",
        "        attn_state_arr.append(attn_state)\n",
        "        # print(attn_state)\n",
        "        decoder_concat_input = Concatenate(axis=-1)([dec_outputs, atten_op])\n",
        "        decoder_concat_input = decoder_dense(decoder_concat_input)\n",
        "        \n",
        "        sampled_token_index = np.argmax( decoder_concat_input[0, -1, :] )\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "        # print(decoded_sentence)\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if sampled_char == \"\\n\" or len(decoded_sentence) > MAX_LEN_target:\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = target_token_index[sampled_char]\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h,c]\n",
        "\n",
        "    return decoded_sentence , attn_state_arr\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3TJR-VvSNig"
      },
      "source": [
        "decode_results = []\n",
        "test_input = None\n",
        "def test_acc_attention(m_name=\"LSTM\" ,latent_dim = 100, n_e_layers = 1,n_d_layers = 1):\n",
        "  global decode_results ,test_input\n",
        "  x_test ,x ,y_test = load_test_data()\n",
        "  test_input = x\n",
        "  enc ,dec  , atten , dense = enc_dec_attention(m_name , n_e_layers ,n_d_layers, latent_dim)\n",
        "  score = 0 \n",
        "  print(len(y_test))\n",
        "\n",
        "  for seq_index in range(len(y_test)):\n",
        "    \n",
        "    input_seq = x_test[seq_index : seq_index + 1]\n",
        "    decoded_sentence , _ = decode_sequence_attention(input_seq,enc,dec,dense,atten)\n",
        "    decode_results.append(decoded_sentence)\n",
        "    if (y_test[seq_index] == decoded_sentence):\n",
        "      score += 1\n",
        "    # print(\"-\")\n",
        "    # print(\"Input sentence:\", y_test[seq_index])\n",
        "    # print(\"Decoded sentence:\", decoded_sentence)\n",
        "  print(score/len(y_test))\n",
        "  return score/len(y_test)\n",
        "test_acc_attention(\"LSTM\" , 128,1,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2f69Z4nqG4t"
      },
      "source": [
        "import pandas as pd\n",
        "def save_csv():\n",
        "  global test_input , decode_results\n",
        "  dict = {\"Input Sentence\" : test_input[:len(decode_results)] , \"Decoded Sentence\" : decode_results}\n",
        "  df = pd.DataFrame(dict)\n",
        "  df.to_csv(\"predictions_attention.csv\")\n",
        "save_csv()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VbUW4odsjsR"
      },
      "source": [
        "## plot attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zg6E5DH_Ok6L",
        "outputId": "d762e0d3-b7c0-4195-ea06-f81e993c1f45"
      },
      "source": [
        "!wget 'https://hindityping.info/download/assets/gujarati-fonts-unicode/Aakar.ttf'"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-26 04:34:11--  https://hindityping.info/download/assets/gujarati-fonts-unicode/Aakar.ttf\n",
            "Resolving hindityping.info (hindityping.info)... 45.76.163.133, 2001:19f0:4400:5287:5400:3ff:fe29:fdbd\n",
            "Connecting to hindityping.info (hindityping.info)|45.76.163.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 77096 (75K) [application/font-TTF]\n",
            "Saving to: ‘Aakar.ttf’\n",
            "\n",
            "Aakar.ttf           100%[===================>]  75.29K   168KB/s    in 0.4s    \n",
            "\n",
            "2021-05-26 04:34:12 (168 KB/s) - ‘Aakar.ttf’ saved [77096/77096]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 785
        },
        "id": "OEfjfXSFyvkX",
        "outputId": "5a74309f-2d07-4740-c83c-91ed2dfb9ddb"
      },
      "source": [
        "import matplotlib as mpl\n",
        "import matplotlib.font_manager as fm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "path = 'Aakar.ttf'\n",
        "fontprop = fm.FontProperties(fname=path, size= 15)\n",
        "\n",
        "def attention_heat_map(m_name=\"LSTM\", latent_dim =100 ,n_e_layers = 1 ,n_d_layers = 1):\n",
        "  x_test , x ,y_test = load_test_data()\n",
        "  enc ,dec  , atten , dense = enc_dec_attention(m_name,n_e_layers ,n_d_layers, latent_dim)\n",
        "  score = 0 \n",
        "  # print(len(y_test))\n",
        "  # plt.figure(figsize = (30,30))\n",
        "  fig, ax = plt.subplots(3,3)\n",
        "  \n",
        "  # fig.figure(figsize = (20,20))\n",
        "  # fig.suptitle(\"Attention Heatmaps\")\n",
        "  \n",
        "  count = 0\n",
        "  for seq_index in range(9):\n",
        "    input_seq = x_test[seq_index : seq_index + 1]\n",
        "    decoded_sentence , attn_states = decode_sequence_attention(input_seq,enc,dec,dense,atten)\n",
        "     \n",
        "    c_names = list(x[seq_index])\n",
        "    c_names[-1] = \"\\\\n\"\n",
        "    # print(c_names)\n",
        "    i_names = list(decoded_sentence)\n",
        "    i_names[-1] = \"\\\\n\"\n",
        "    # print(i_names,c_names)\n",
        "    temp = np.shape(attn_states)\n",
        "    # print(temp)\n",
        "    attn_states = np.reshape(attn_states,(temp[0],temp[-1]))[:,:len(c_names)]\n",
        "    \n",
        "    temp1 = count//3\n",
        "    temp2 = count%3\n",
        "    count += 1\n",
        "    fig.set_figwidth(10)\n",
        "    fig.set_figheight(10)\n",
        "    cax = ax[temp1][temp2].imshow(np.array(attn_states))\n",
        "    ax[temp1][temp2].set_xticks(np.arange(len(c_names)))\n",
        "    ax[temp1][temp2].set_yticks(np.arange(len(i_names)))\n",
        "    ax[temp1][temp2].set_xticklabels(c_names)\n",
        "    ax[temp1][temp2].set_yticklabels(i_names, fontproperties = fontprop)\n",
        "\n",
        "    \n",
        "  \n",
        "  fig.tight_layout()\n",
        "  plt.savefig('confmatrix.png', dpi=600,bbox_inches='tight')\n",
        "  return \n",
        "attention_heat_map(\"LSTM\",128,1,1)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10373, 23)\n",
            "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 9 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Tg-XeJdwSo6"
      },
      "source": [
        "# visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwCL2BnMwW6C"
      },
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "import re\n",
        "\n",
        "# Imports for visualisations\n",
        "from IPython.display import HTML as html_print\n",
        "from IPython.display import display\n",
        "import keras.backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ga0khc74xGGN"
      },
      "source": [
        "# get html element\n",
        "def cstr(s, color='black'):\n",
        "\tif s == ' ':\n",
        "\t\treturn \"<text style=color:#000;padding-left:10px;background-color:{}> </text>\".format(color, s)\n",
        "\telse:\n",
        "\t\treturn \"<text style=color:#000;background-color:{}>{} </text>\".format(color, s)\n",
        "\t\n",
        "# print html\n",
        "def print_color(t):\n",
        "\tdisplay(html_print(''.join([cstr(ti, color=ci) for ti,ci in t])))\n",
        "\n",
        "# get appropriate color for value\n",
        "def get_clr(value):\n",
        "  colors = ['#85c2e1', '#89c4e2', '#95cae5', '#99cce6', '#a1d0e8'\n",
        "\t\t'#b2d9ec', '#baddee', '#c2e1f0', '#eff7fb', '#f9e8e8',\n",
        "\t\t'#f9e8e8', '#f9d4d4', '#f9bdbd', '#f8a8a8', '#f68f8f',\n",
        "\t\t'#f47676', '#f45f5f', '#f34343', '#f33b3b', '#f42e2e']\n",
        "  l = len(colors)\n",
        "  value = int((value * 100) / 5)\n",
        "  if (value >= l):\n",
        "    value = l-1\n",
        "  \n",
        "  return colors[value]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "TZhXeSFWUCri",
        "outputId": "544610e8-ac75-4373-fd96-c4ef2d905f8b"
      },
      "source": [
        "from google.colab import output\n",
        "import time\n",
        "def visualize(m_name=\"LSTM\", latent_dim =100 ,n_e_layers = 1 ,n_d_layers = 1):\n",
        "  x_test , x ,y_test = load_test_data()\n",
        "  enc ,dec  , atten , dense = enc_dec_attention(m_name,n_e_layers ,n_d_layers, latent_dim)\n",
        "  \n",
        "  # print(len(y_test)) 65 71 104\n",
        "  for seq_index in range(71,72):\n",
        "    input_seq = x_test[seq_index : seq_index + 1]\n",
        "    decoded_sentence , attn_states = decode_sequence_attention(input_seq,enc,dec,dense,atten)\n",
        "    # print(len(x[seq_index]),len(decoded_sentence))\n",
        "    # print(np.shape(attn_states))\n",
        "    temp = np.shape(attn_states)\n",
        "    attn_states =  np.array(attn_states).reshape((temp[0],temp[-1]))[:-1,:len(x[seq_index])-1]\n",
        "    # print(attn_states)\n",
        "    \n",
        "    # print(np.shape(attn_states))\n",
        "    \n",
        "    for j in range(len(attn_states)):\n",
        "      text_color = [(x[seq_index][i],get_clr(attn_states[j][i])) for i in range(len(x[seq_index])-1)]\n",
        "      output.clear()\n",
        "      print(decoded_sentence[j])\n",
        "      print_color(text_color)\n",
        "      time.sleep(1.5)\n",
        "      # output.clear()\n",
        "\n",
        "  return \n",
        "visualize(\"LSTM\",128,1,1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ર\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<text style=color:#000;background-color:#85c2e1>a </text><text style=color:#000;background-color:#85c2e1>k </text><text style=color:#000;background-color:#85c2e1>s </text><text style=color:#000;background-color:#85c2e1>h </text><text style=color:#000;background-color:#85c2e1>a </text><text style=color:#000;background-color:#f45f5f>r </text>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}