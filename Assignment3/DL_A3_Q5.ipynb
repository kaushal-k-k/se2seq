{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_A3_Q5.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "uRGyFIT_axmv",
        "NlX_ycI-a3cD"
      ],
      "authorship_tag": "ABX9TyNnmLhQ17EEXE1KqMr6v9M5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRGyFIT_axmv"
      },
      "source": [
        "# download Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IntdRGraoSN",
        "outputId": "f6a97d81-dfb2-4471-8077-2e113b541f14"
      },
      "source": [
        "!wget https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
        "!tar -xvf  'dakshina_dataset_v1.0.tar'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-17 11:57:42--  https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.142.128, 74.125.195.128, 142.250.107.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.142.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2008340480 (1.9G) [application/x-tar]\n",
            "Saving to: ‘dakshina_dataset_v1.0.tar’\n",
            "\n",
            "dakshina_dataset_v1 100%[===================>]   1.87G   124MB/s    in 12s     \n",
            "\n",
            "2021-05-17 11:57:53 (163 MB/s) - ‘dakshina_dataset_v1.0.tar’ saved [2008340480/2008340480]\n",
            "\n",
            "dakshina_dataset_v1.0/bn/\n",
            "dakshina_dataset_v1.0/bn/lexicons/\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/romanized/\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/gu/\n",
            "dakshina_dataset_v1.0/gu/lexicons/\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/romanized/\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/hi/\n",
            "dakshina_dataset_v1.0/hi/lexicons/\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/hi/romanized/\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/kn/\n",
            "dakshina_dataset_v1.0/kn/lexicons/\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/kn/romanized/\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ml/\n",
            "dakshina_dataset_v1.0/ml/lexicons/\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/tmp.rom.txt\n",
            "dakshina_dataset_v1.0/ml/lexicons/tmp.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/romanized/\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/mr/\n",
            "dakshina_dataset_v1.0/mr/lexicons/\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/romanized/\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/pa/\n",
            "dakshina_dataset_v1.0/pa/lexicons/\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/pa/romanized/\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/sd/\n",
            "dakshina_dataset_v1.0/sd/lexicons/\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/sd/romanized/\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/si/\n",
            "dakshina_dataset_v1.0/si/lexicons/\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/romanized/\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ta/\n",
            "dakshina_dataset_v1.0/ta/lexicons/\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/romanized/\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/te/\n",
            "dakshina_dataset_v1.0/te/lexicons/\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/romanized/\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ur/\n",
            "dakshina_dataset_v1.0/ur/lexicons/\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/romanized/\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/README.md\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlX_ycI-a3cD"
      },
      "source": [
        "# load data and process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jyB-MXYatP8"
      },
      "source": [
        "%pip install wandb -q\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-9k_tSca8-P"
      },
      "source": [
        "input_token_index = None\n",
        "target_token_index = None\n",
        "MAX_LEN_input = None\n",
        "MAX_LEN_target = None\n",
        "num_encoder_tokens = 30\n",
        "num_decoder_tokens = 70\n",
        "input_tokenizer = None\n",
        "target_tokenizer = None"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4KPNp7GbBZW"
      },
      "source": [
        "def tokenize(data,vocab_size):\n",
        "  tokenizer = Tokenizer(num_words=vocab_size,char_level=True)\n",
        "  tokenizer.fit_on_texts(data)\n",
        "  temp=tokenizer.texts_to_sequences(data)\n",
        "  # print(data[:3])\n",
        "  # print(temp[:3])\n",
        "  dictionary = tokenizer.word_index\n",
        "  return temp , dictionary , tokenizer"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qacRMXz9bCPu"
      },
      "source": [
        "def load_and_preprocess():\n",
        "  global input_token_index , target_token_index , MAX_LEN_input , MAX_LEN_target ,num_decoder_tokens,num_encoder_tokens , input_tokenizer , target_tokenizer\n",
        "  data_path = 'dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.train.tsv'\n",
        "  input_texts = []\n",
        "  target_texts = []\n",
        "  with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.read().split(\"\\n\")\n",
        "  for line in lines[:-1]:\n",
        "    temp = line.split('\\t')\n",
        "    input_text, target_text = temp[1],temp[0] \n",
        "    target_text = \"\\t\" + target_text + \"\\n\"\n",
        "    input_text = input_text+\"\\n\"\n",
        "\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "  \n",
        "  MAX_LEN_input = max([len(txt) for txt in input_texts])\n",
        "  MAX_LEN_target = max([len(txt) for txt in target_texts])\n",
        "\n",
        "  # toeknize\n",
        "  encoder_input , input_token_index , input_tokenizer = tokenize(input_texts , num_encoder_tokens)\n",
        "  decoder_input , target_token_index, target_tokenizer = tokenize(target_texts , num_decoder_tokens) \n",
        "\n",
        "  # padding\n",
        "  encoder_input_data = pad_sequences(encoder_input, maxlen=MAX_LEN_input, dtype='int32', padding='post', truncating='post',value= input_token_index[\"\\n\"])\n",
        "  decoder_input_data = pad_sequences(decoder_input, maxlen=MAX_LEN_target, dtype='int32', padding='post', truncating='post',value=target_token_index[\"\\n\"])\n",
        "\n",
        "  decoder_target_data = np.zeros((len(input_texts), MAX_LEN_target, num_decoder_tokens), dtype=\"float32\")\n",
        "  for i,  target_text in enumerate(target_texts):\n",
        "    for t, char in enumerate(target_text):\n",
        "      if t > 0:\n",
        "        decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "    decoder_target_data[i, t:, target_token_index[\"\\n\"]] = 1.0\n",
        "\n",
        "  return encoder_input_data , decoder_input_data, decoder_target_data\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNOUxbiydfvj"
      },
      "source": [
        "def load_val_data(data_path = 'dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.dev.tsv'):\n",
        "  global input_token_index , target_token_index , MAX_LEN_input , MAX_LEN_target ,num_decoder_tokens,num_encoder_tokens , input_tokenizer , target_tokenizer\n",
        "  \n",
        "  input_texts = []\n",
        "  target_texts = []\n",
        "  with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.read().split(\"\\n\")\n",
        "  for line in lines[:-1]:\n",
        "    temp = line.split('\\t')\n",
        "    input_text, target_text = temp[1],temp[0] \n",
        "    target_text = \"\\t\" + target_text + \"\\n\"\n",
        "    input_text = input_text+\"\\n\"\n",
        "\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "  \n",
        "  # tokenize\n",
        "  encoder_input  = input_tokenizer.texts_to_sequences(input_texts)\n",
        "  decoder_input  = target_tokenizer.texts_to_sequences(target_texts) \n",
        "\n",
        "  # padding\n",
        "  encoder_input_data = pad_sequences(encoder_input, maxlen=MAX_LEN_input, dtype='int32', padding='post', truncating='post',value= input_token_index[\"\\n\"])\n",
        "  decoder_input_data = pad_sequences(decoder_input, maxlen=MAX_LEN_target, dtype='int32', padding='post', truncating='post',value=target_token_index[\"\\n\"])\n",
        "\n",
        "  decoder_target_data = np.zeros((len(input_texts), MAX_LEN_target, num_decoder_tokens), dtype=\"float32\")\n",
        "  for i,  target_text in enumerate(target_texts):\n",
        "    for t, char in enumerate(target_text):\n",
        "      if char == 'ૠ':\n",
        "        char = 'ઋ'\n",
        "      if t > 0:\n",
        "        decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "    decoder_target_data[i, t:, target_token_index[\"\\n\"]] = 1.0\n",
        "\n",
        "  return encoder_input_data , decoder_input_data, decoder_target_data\n",
        "\n",
        "# load_val_data()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e3vrfyUcDN_"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow.python.keras.layers import Layer, Concatenate\n",
        "from tensorflow.python.keras import backend as K\n",
        "\n",
        "\n",
        "class AttentionLayer(Layer):\n",
        "    \"\"\"\n",
        "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
        "    There are three sets of weights introduced W_a, U_a, and V_a\n",
        "     \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert isinstance(input_shape, list)\n",
        "        # Create a trainable weight variable for this layer.\n",
        "\n",
        "        self.W_a = self.add_weight(name='W_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.U_a = self.add_weight(name='U_a',\n",
        "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.V_a = self.add_weight(name='V_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "\n",
        "        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "    def call(self, inputs, verbose=False):\n",
        "        \"\"\"\n",
        "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
        "        \"\"\"\n",
        "        assert type(inputs) == list\n",
        "        encoder_out_seq, decoder_out_seq = inputs\n",
        "        if verbose:\n",
        "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
        "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
        "\n",
        "        def energy_step(inputs, states):\n",
        "            \"\"\" Step function for computing energy for a single decoder state\n",
        "            inputs: (batchsize * 1 * de_in_dim)\n",
        "            states: (batchsize * 1 * de_latent_dim)\n",
        "            \"\"\"\n",
        "\n",
        "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n",
        "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
        "\n",
        "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
        "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
        "            de_hidden = inputs.shape[-1]\n",
        "\n",
        "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
        "            # <= batch size * en_seq_len * latent_dim\n",
        "            W_a_dot_s = K.dot(encoder_out_seq, self.W_a)\n",
        "\n",
        "            \"\"\" Computing hj.Ua \"\"\"\n",
        "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
        "            if verbose:\n",
        "                print('Ua.h>', U_a_dot_h.shape)\n",
        "\n",
        "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            Ws_plus_Uh = K.tanh(W_a_dot_s + U_a_dot_h)\n",
        "            if verbose:\n",
        "                print('Ws+Uh>', Ws_plus_Uh.shape)\n",
        "\n",
        "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.squeeze(K.dot(Ws_plus_Uh, self.V_a), axis=-1)\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.softmax(e_i)\n",
        "\n",
        "            if verbose:\n",
        "                print('ei>', e_i.shape)\n",
        "\n",
        "            return e_i, [e_i]\n",
        "\n",
        "        def context_step(inputs, states):\n",
        "            \"\"\" Step function for computing ci using ei \"\"\"\n",
        "\n",
        "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n",
        "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
        "\n",
        "            # <= batch_size, hidden_size\n",
        "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
        "            if verbose:\n",
        "                print('ci>', c_i.shape)\n",
        "            return c_i, [c_i]\n",
        "\n",
        "        fake_state_c = K.sum(encoder_out_seq, axis=1)\n",
        "        fake_state_e = K.sum(encoder_out_seq, axis=2)  # <= (batch_size, enc_seq_len, latent_dim\n",
        "\n",
        "        \"\"\" Computing energy outputs \"\"\"\n",
        "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
        "        last_out, e_outputs, _ = K.rnn(\n",
        "            energy_step, decoder_out_seq, [fake_state_e],\n",
        "        )\n",
        "\n",
        "        \"\"\" Computing context vectors \"\"\"\n",
        "        last_out, c_outputs, _ = K.rnn(\n",
        "            context_step, e_outputs, [fake_state_c],\n",
        "        )\n",
        "\n",
        "        return c_outputs, e_outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \"\"\" Outputs produced by the layer \"\"\"\n",
        "        return [\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
        "        ]\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sd0yQw9ubKFK"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense,LSTM,Input,GRU,SimpleRNN,Dropout,Embedding\n",
        "\n",
        "def create_model_attention(m_name=\"LSTM\",n_e_layers=1,n_d_layers=1,latent_dim = 100,embedding_size = 16,dropout = 0 , recurrent_dropout = 0):\n",
        "  global num_decoder_tokens,num_decoder_tokens\n",
        "  keras.backend.clear_session()\n",
        "  \n",
        "  # Define an input sequence and process it.\n",
        "  input1 = Input(shape=(None,),name= \"input_1\")\n",
        "  encoder_inputs = Embedding(input_dim = num_encoder_tokens, output_dim = embedding_size)(input1)\n",
        "\n",
        "  encoder = globals()[m_name](latent_dim,dropout=dropout,recurrent_dropout = recurrent_dropout, return_state=True,return_sequences=True)\n",
        "  e_o = encoder(encoder_inputs)\n",
        "  prev = e_o\n",
        "  for i in range(1,n_e_layers):\n",
        "    e = globals()[m_name](latent_dim, dropout=dropout,recurrent_dropout = recurrent_dropout,return_state=True,return_sequences=True)\n",
        "    e_o = e(prev[0])\n",
        "    prev = e_o\n",
        "  \n",
        "  input2 = Input(shape=(None,),name=\"input_2\")\n",
        "  decoder_inputs = Embedding(input_dim = num_decoder_tokens, output_dim = embedding_size)(input2)\n",
        "  d_l = globals()[m_name](latent_dim,dropout=dropout,recurrent_dropout = recurrent_dropout, return_sequences=True, return_state=True)(decoder_inputs,initial_state = e_o[1:])\n",
        "  p_d = d_l[0]\n",
        "  for i in range(1,n_d_layers):\n",
        "    d_l = globals()[m_name](latent_dim,dropout=dropout,recurrent_dropout = recurrent_dropout, return_state=True, return_sequences=True)(p_d,initial_state = e_o[1:])\n",
        "    p_d = d_l[0]\n",
        "\n",
        "  attn_layer = AttentionLayer(name=\"attention_layer\")\n",
        "  attn_op, attn_state = attn_layer([e_o[0], d_l[0]])\n",
        "  decoder_concat_input = Concatenate(axis=-1)([d_l[0], attn_op])\n",
        "\n",
        "\n",
        "  dec_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "  final_output = dec_dense(decoder_concat_input)\n",
        " \n",
        "\n",
        "  # Define the model that will turn\n",
        "  # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "  model = keras.Model([input1,input2], final_output)\n",
        "\n",
        "  return model\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfOXyhI9eGCB"
      },
      "source": [
        "# sweep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M21Ip4O-bNDb"
      },
      "source": [
        "def train_attention():\n",
        "  run = wandb.init()\n",
        "  c = run.config\n",
        "  name = \"model_\"+c.model+\"_o_\"+c.optimizer+\"_hs_\"+str(c.hidden_size)+\"_em_\"+str(c.embedding_size)+\"_d_\"+str(c.dropout)\n",
        "  run.name = name\n",
        "  print(name)\n",
        "  batch_size = 128\n",
        "  epochs = 1\n",
        "\n",
        "  # used single encoder and decoder layer\n",
        "  encoder_layers , decoder_layers = 1 , 1\n",
        "\n",
        "\n",
        "  encoder_input_data,decoder_input_data ,decoder_target_data = load_and_preprocess()\n",
        "  val_encoder_input_data,val_decoder_input_data ,val_decoder_target_data = load_val_data()\n",
        "\n",
        "  es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
        "\n",
        "  model = create_model_attention(c.model,encoder_layers,decoder_layers,c.hidden_size,c.embedding_size,c.dropout,0)\n",
        "  model.compile(optimizer=c.optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "  model.fit(\n",
        "    [encoder_input_data, decoder_input_data],\n",
        "    decoder_target_data,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=([val_encoder_input_data, val_decoder_input_data],val_decoder_target_data),\n",
        "    # validation_split=0.2,\n",
        "    callbacks=[WandbCallback(),es]\n",
        "  )\n",
        "  \n",
        "  temp = model.predict([val_encoder_input_data, val_decoder_input_data]).argmax(axis=-1)\n",
        "  val_word_acc = sum((temp[:,:-1] == val_decoder_input_data[:,1:]).all(axis=-1)) / len(val_encoder_input_data)\n",
        "  # print(val_word_acc)\n",
        "\n",
        "  temp = model.predict([encoder_input_data,decoder_input_data]).argmax(axis=-1)\n",
        "  train_word_acc = sum((temp[:,:-1] == decoder_input_data[:,1:]).all(axis=-1)) / len(encoder_input_data)\n",
        "  # print(train_word_acc)\n",
        "\n",
        "  wandb.log({\"val_word_acc\" : round(val_word_acc,4) , \"train_word_acc\" : round(train_word_acc,4)})\n",
        "  return"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBLtbSTMeV4W"
      },
      "source": [
        "sweep_config_attention={\n",
        "    'method' : 'random' ,\n",
        "    'metric' : { 'name' : 'val_word_acc' , 'goal' : 'maximize' } ,\n",
        "    'parameters' : {\n",
        "        'model' : { 'values' : ['LSTM','GRU','SimpleRNN'] },\n",
        "        'dropout' : { 'values' : [0.1,0.2]},\n",
        "        'embedding_size' : {'values' : [32,64,128]},\n",
        "        'hidden_size' : {'values' : [128,256,512]},\n",
        "        'optimizer' : {'values' : ['rmsprop' ,'adam']}\n",
        "    }\n",
        "}\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "MXcJYQLriFNT",
        "outputId": "600f17c9-6c2c-43e0-c4c0-c5c27d1cc126"
      },
      "source": [
        "\n",
        "sweepid = wandb.sweep(sweep_config_attention,project=\"DL_A3_Q5_testing1\",entity =\"sonagara\")\n",
        "wandb.agent(sweepid,train_attention)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Create sweep with ID: vyq5gxxv\n",
            "Sweep URL: https://wandb.ai/sonagara/DL_A3_Q5_testing1/sweeps/vyq5gxxv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cqxawoxc with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel: LSTM\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.30<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">iconic-sweep-1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/sonagara/DL_A3_Q5_testing1\" target=\"_blank\">https://wandb.ai/sonagara/DL_A3_Q5_testing1</a><br/>\n",
              "                Sweep page: <a href=\"https://wandb.ai/sonagara/DL_A3_Q5_testing1/sweeps/vyq5gxxv\" target=\"_blank\">https://wandb.ai/sonagara/DL_A3_Q5_testing1/sweeps/vyq5gxxv</a><br/>\n",
              "Run page: <a href=\"https://wandb.ai/sonagara/DL_A3_Q5_testing1/runs/cqxawoxc\" target=\"_blank\">https://wandb.ai/sonagara/DL_A3_Q5_testing1/runs/cqxawoxc</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210517_120624-cqxawoxc</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "model_LSTM_o_adam_hs_128_em_32_d_0.2\n",
            "131/823 [===>..........................] - ETA: 4:09 - loss: 2.2198 - accuracy: 0.6426"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAp20x7IeUBF"
      },
      "source": [
        "#  best model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWjs9FCneYkb"
      },
      "source": [
        "def train_best():\n",
        "  m_name = \"LSTM\"\n",
        "  encoder_layers = 1\n",
        "  decoder_layers = 1\n",
        "  latent_dim = 512\n",
        "  embedding_size = 64\n",
        "  dropout = 0.2\n",
        "  batch_size = 64\n",
        "  recurrent_dropout = 0  # 0 to use cudnnlstm which is faster than lstm\n",
        "\n",
        "  encoder_input_data,decoder_input_data ,decoder_target_data = load_and_preprocess()\n",
        "  val_encoder_input_data,val_decoder_input_data ,val_decoder_target_data = load_val_data()\n",
        "  \n",
        "\n",
        "  model = create_model_attention(m_name,encoder_layers,decoder_layers,latent_dim,embedding_size,dropout,recurrent_dropout)\n",
        "  # model = keras.models.load_model(\"s2s\")\n",
        "  model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "  es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
        "\n",
        "  model.fit(\n",
        "    [encoder_input_data, decoder_input_data],\n",
        "    decoder_target_data,\n",
        "    batch_size=batch_size,\n",
        "    epochs=20,\n",
        "    # validation_split = 0.2,\n",
        "    validation_data=([val_encoder_input_data, val_decoder_input_data],val_decoder_target_data),\n",
        "    callbacks=[es]\n",
        "  )\n",
        "\n",
        "  temp = model.predict([val_encoder_input_data, val_decoder_input_data]).argmax(axis=-1)\n",
        "  val_word_acc = sum((temp[:,:-1] == val_decoder_input_data[:,1:]).all(axis=-1)) / len(val_encoder_input_data)\n",
        "  print(val_word_acc)\n",
        "  \n",
        "\n",
        "\n",
        "  # Save model\n",
        "  model.save(\"s2sa\")\n",
        "  # print(test_acc(\"LSTM\",100,1,2))\n",
        "train_best()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRCAPV3yHXZo"
      },
      "source": [
        "def enc_dec_attention(m_name=\"LSTM\",n_e_layers=1,n_d_layers=1,latent_dim = 100):\n",
        "  model = keras.models.load_model(\"s2sa\")\n",
        " \n",
        "  if (n_e_layers == 1):\n",
        "    l_name = \"\"\n",
        "  else:\n",
        "    l_name = \"_\"+str(n_e_layers-1)\n",
        "\n",
        "  if (m_name == \"SimpleRNN\"):\n",
        "    n_name = \"simple_rnn\"\n",
        "  else:\n",
        "    n_name = m_name\n",
        "  # model.summary()\n",
        "\n",
        "  # encoder\n",
        "  encoder_inputs = model.input[0]\n",
        "  encoder_outputs, *encoder_states = model.get_layer(n_name.lower()+l_name).output  # last encoding layer\n",
        "  encoder_model = tf.keras.models.Model(encoder_inputs, [encoder_outputs,encoder_states])\n",
        "  # encoder_model.summary()\n",
        "    \n",
        "\n",
        "  # decoder\n",
        "  decoder_inputs = model.input[1]\n",
        "  decoder_embed = model.get_layer(\"embedding_1\")(decoder_inputs)\n",
        "  decoder_state_input_h = tf.keras.layers.Input(shape=(latent_dim,))\n",
        "  decoder_state_input_c = tf.keras.layers.Input(shape=(latent_dim,))\n",
        "  decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "  decoder_outputs, *decoder_states = model.get_layer(n_name.lower()+\"_\"+str(n_e_layers))(decoder_embed, initial_state=decoder_states_inputs)\n",
        "  decoder_model = tf.keras.models.Model([decoder_inputs, decoder_states_inputs],[decoder_outputs] + decoder_states)\n",
        "  # decoder_model.summary()\n",
        "\n",
        "  atten_layer = model.get_layer(\"attention_layer\")\n",
        "  dense_layer = model.get_layer(\"dense\")\n",
        "\n",
        "  return encoder_model , decoder_model ,atten_layer ,dense_layer \n",
        "\n",
        "# enc_dec_attention()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHlT86NlOfRF"
      },
      "source": [
        "\n",
        "# Reverse-lookup token index to decode sequences back to\n",
        "# something readable.\n",
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
        "\n",
        "\n",
        "def decode_sequence_attention(input_seq,encoder_model,decoder_model,decoder_dense,atten_layer):\n",
        "    global num_decoder_tokens , target_token_index , reverse_target_char_index , MAX_LEN_target\n",
        "    # Encode the input as state vectors.\n",
        "\n",
        "    enc_op, states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0] = target_token_index[\"\\t\"]\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = \"\"\n",
        "    while not stop_condition:\n",
        "        dec_outputs, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        atten_op, attn_state = atten_layer([enc_op, dec_outputs])\n",
        "        # print(attn_state)\n",
        "        decoder_concat_input = Concatenate(axis=-1)([dec_outputs, atten_op])\n",
        "        decoder_concat_input = decoder_dense(decoder_concat_input)\n",
        "        \n",
        "        sampled_token_index = np.argmax( decoder_concat_input[0, -1, :] )\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "        # print(decoded_sentence)\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if sampled_char == \"\\n\" or len(decoded_sentence) > MAX_LEN_target:\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = target_token_index[sampled_char]\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "r3TJR-VvSNig",
        "outputId": "5bda4a61-f8d4-47a8-8e3c-c13914245ca9"
      },
      "source": [
        "def test_acc_attention(m_name=\"LSTM\" ,latent_dim = 100, n_e_layers = 1,n_d_layers = 1):\n",
        "  x_test , y_test = load_test_data()\n",
        "  enc ,dec  , atten , dense = enc_dec_attention(m_name , n_e_layers ,n_d_layers, latent_dim)\n",
        "  score = 0 \n",
        "  print(len(y_test))\n",
        "\n",
        "  for seq_index in range(len(y_test)):\n",
        "    \n",
        "    input_seq = x_test[seq_index : seq_index + 1]\n",
        "    decoded_sentence = decode_sequence_attention(input_seq,enc,dec,dense,atten)\n",
        "    if (y_test[seq_index] == decoded_sentence):\n",
        "      score += 1\n",
        "    print(\"-\")\n",
        "    print(\"Input sentence:\", y_test[seq_index])\n",
        "    print(\"Decoded sentence:\", decoded_sentence)\n",
        "  print(score/len(y_test))\n",
        "  return score/len(y_test)\n",
        "test_acc_attention()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10373, 19)\n",
            "10373\n",
            "tf.Tensor(\n",
            "[[[0.03299213 0.03293254 0.03743091 0.0370109  0.04104772 0.04129481\n",
            "   0.04086666 0.04202858 0.04510744 0.05038547 0.05707331 0.06273546\n",
            "   0.06598404 0.06765498 0.06856006 0.06901259 0.06922045 0.06931201\n",
            "   0.06935   ]]], shape=(1, 1, 19), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[0.03760805 0.03796143 0.04199007 0.04221893 0.04535198 0.04603993\n",
            "   0.04594486 0.04710242 0.04956936 0.05316092 0.0570058  0.05984121\n",
            "   0.06133004 0.06203482 0.06238378 0.06254701 0.0626171  0.06264339\n",
            "   0.06264894]]], shape=(1, 1, 19), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[0.04384023 0.04415859 0.04677034 0.04703319 0.04888453 0.04938265\n",
            "   0.04935042 0.05003621 0.05142121 0.05331973 0.05522535 0.056564\n",
            "   0.05725317 0.05758072 0.05774564 0.05782426 0.05785898 0.05787314\n",
            "   0.05787772]]], shape=(1, 1, 19), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[0.04347179 0.04408089 0.04678016 0.04743042 0.04911705 0.04993662\n",
            "   0.05005114 0.05086787 0.05227703 0.05398079 0.05548227 0.0564263\n",
            "   0.05687838 0.05708118 0.05717786 0.05722226 0.05724104 0.05724789\n",
            "   0.05724906]]], shape=(1, 1, 19), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[0.04602812 0.04661225 0.04857847 0.04919649 0.05028662 0.05097956\n",
            "   0.05111719 0.05174644 0.05274594 0.05385586 0.05473434 0.05522477\n",
            "   0.05543707 0.05552559 0.05556599 0.05558414 0.05559175 0.05559453\n",
            "   0.05559487]]], shape=(1, 1, 19), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[0.04167213 0.04250547 0.04563992 0.04657116 0.04845106 0.04955659\n",
            "   0.0497622  0.05080148 0.05251078 0.05449089 0.05614233 0.0571175\n",
            "   0.05755977 0.05775173 0.05784348 0.05788663 0.05790588 0.05791402\n",
            "   0.05791705]]], shape=(1, 1, 19), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[0.03357768 0.03510889 0.03979322 0.04171383 0.04442596 0.04671256\n",
            "   0.04726087 0.04938162 0.05277839 0.05659752 0.05952109 0.06097942\n",
            "   0.06150685 0.06168681 0.06175747 0.06178725 0.06179978 0.06180472\n",
            "   0.06180608]]], shape=(1, 1, 19), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[0.02205164 0.02375651 0.02942509 0.03205866 0.03590889 0.03952569\n",
            "   0.04044083 0.04407717 0.05040237 0.05831791 0.06495455 0.06839011\n",
            "   0.06960825 0.07000248 0.07014908 0.07020891 0.07023345 0.07024293\n",
            "   0.07024547]]], shape=(1, 1, 19), dtype=float32)\n",
            "-\n",
            "Input sentence: ankanu\n",
            "Decoded sentence: anaanee\n",
            "\n",
            "tf.Tensor(\n",
            "[[[0.03299213 0.03293254 0.03743091 0.0370109  0.04104772 0.04129481\n",
            "   0.04086666 0.04202858 0.04510744 0.05038547 0.05707331 0.06273546\n",
            "   0.06598404 0.06765498 0.06856006 0.06901259 0.06922045 0.06931201\n",
            "   0.06935   ]]], shape=(1, 1, 19), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[0.03760805 0.03796143 0.04199007 0.04221893 0.04535198 0.04603993\n",
            "   0.04594486 0.04710242 0.04956936 0.05316092 0.0570058  0.05984121\n",
            "   0.06133004 0.06203482 0.06238378 0.06254701 0.0626171  0.06264339\n",
            "   0.06264894]]], shape=(1, 1, 19), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[0.04384023 0.04415859 0.04677034 0.04703319 0.04888453 0.04938265\n",
            "   0.04935042 0.05003621 0.05142121 0.05331973 0.05522535 0.056564\n",
            "   0.05725317 0.05758072 0.05774564 0.05782426 0.05785898 0.05787314\n",
            "   0.05787772]]], shape=(1, 1, 19), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[0.04347179 0.04408089 0.04678016 0.04743042 0.04911705 0.04993662\n",
            "   0.05005114 0.05086787 0.05227703 0.05398079 0.05548227 0.0564263\n",
            "   0.05687838 0.05708118 0.05717786 0.05722226 0.05724104 0.05724789\n",
            "   0.05724906]]], shape=(1, 1, 19), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[0.04602812 0.04661225 0.04857847 0.04919649 0.05028662 0.05097956\n",
            "   0.05111719 0.05174644 0.05274594 0.05385586 0.05473434 0.05522477\n",
            "   0.05543707 0.05552559 0.05556599 0.05558414 0.05559175 0.05559453\n",
            "   0.05559487]]], shape=(1, 1, 19), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[0.04167213 0.04250547 0.04563992 0.04657116 0.04845106 0.04955659\n",
            "   0.0497622  0.05080148 0.05251078 0.05449089 0.05614233 0.0571175\n",
            "   0.05755977 0.05775173 0.05784348 0.05788663 0.05790588 0.05791402\n",
            "   0.05791705]]], shape=(1, 1, 19), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[0.03357768 0.03510889 0.03979322 0.04171383 0.04442596 0.04671256\n",
            "   0.04726087 0.04938162 0.05277839 0.05659752 0.05952109 0.06097942\n",
            "   0.06150685 0.06168681 0.06175747 0.06178725 0.06179978 0.06180472\n",
            "   0.06180608]]], shape=(1, 1, 19), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[0.02205164 0.02375651 0.02942509 0.03205866 0.03590889 0.03952569\n",
            "   0.04044083 0.04407717 0.05040237 0.05831791 0.06495455 0.06839011\n",
            "   0.06960825 0.07000248 0.07014908 0.07020891 0.07023345 0.07024293\n",
            "   0.07024547]]], shape=(1, 1, 19), dtype=float32)\n",
            "-\n",
            "Input sentence: anknu\n",
            "Decoded sentence: anaanee\n",
            "\n",
            "tf.Tensor(\n",
            "[[[0.02857595 0.02855753 0.03343949 0.03307175 0.03798863 0.03673698\n",
            "   0.03726806 0.03963222 0.04438663 0.05160273 0.05947776 0.06541949\n",
            "   0.06904595 0.07108641 0.07212148 0.07261899 0.07286894 0.07300796\n",
            "   0.07309303]]], shape=(1, 1, 19), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[0.03417651 0.0345634  0.03918131 0.03942747 0.04353772 0.04268712\n",
            "   0.04337253 0.04554586 0.04933635 0.05426942 0.05887952 0.06193334\n",
            "   0.06358945 0.06442136 0.06480794 0.06498189 0.06506299 0.06510293\n",
            "   0.06512287]]], shape=(1, 1, 19), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[0.04648023 0.0468509  0.04873428 0.04910084 0.05046022 0.05022522\n",
            "   0.05051202 0.05126005 0.05239761 0.05366165 0.05467447 0.05527539\n",
            "   0.05557899 0.05572215 0.0557846  0.05581068 0.05582111 0.05582473\n",
            "   0.05582484]]], shape=(1, 1, 19), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[0.0435136  0.04404496 0.04676537 0.04731945 0.04932867 0.04897757\n",
            "   0.04939394 0.05049394 0.05218796 0.05409901 0.0556531  0.05658451\n",
            "   0.05706005 0.05729144 0.0573982  0.05744674 0.05747003 0.05748231\n",
            "   0.05748913]]], shape=(1, 1, 19), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[0.04321738 0.0440757  0.04679358 0.04772776 0.04951145 0.04926932\n",
            "   0.04984847 0.05112995 0.05292444 0.05471902 0.05594329 0.0565308\n",
            "   0.05677111 0.05687065 0.05691254 0.05693046 0.05693844 0.05694205\n",
            "   0.05694362]]], shape=(1, 1, 19), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[0.03645559 0.03782028 0.0420422  0.04368374 0.04661271 0.04622972\n",
            "   0.04728515 0.04962162 0.05297343 0.05639457 0.05870584 0.05975569\n",
            "   0.06014985 0.06030171 0.06036339 0.06038936 0.06040085 0.06040604\n",
            "   0.06040825]]], shape=(1, 1, 19), dtype=float32)\n",
            "-\n",
            "Input sentence: ankane\n",
            "Decoded sentence: aanee\n",
            "\n",
            "tf.Tensor(\n",
            "[[[0.02857595 0.02855753 0.03343949 0.03307175 0.03798863 0.03673698\n",
            "   0.03726806 0.03963222 0.04438663 0.05160273 0.05947776 0.06541949\n",
            "   0.06904595 0.07108641 0.07212148 0.07261899 0.07286894 0.07300796\n",
            "   0.07309303]]], shape=(1, 1, 19), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[0.03417651 0.0345634  0.03918131 0.03942747 0.04353772 0.04268712\n",
            "   0.04337253 0.04554586 0.04933635 0.05426942 0.05887952 0.06193334\n",
            "   0.06358945 0.06442136 0.06480794 0.06498189 0.06506299 0.06510293\n",
            "   0.06512287]]], shape=(1, 1, 19), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[0.04648023 0.0468509  0.04873428 0.04910084 0.05046022 0.05022522\n",
            "   0.05051202 0.05126005 0.05239761 0.05366165 0.05467447 0.05527539\n",
            "   0.05557899 0.05572215 0.0557846  0.05581068 0.05582111 0.05582473\n",
            "   0.05582484]]], shape=(1, 1, 19), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[0.0435136  0.04404496 0.04676537 0.04731945 0.04932867 0.04897757\n",
            "   0.04939394 0.05049394 0.05218796 0.05409901 0.0556531  0.05658451\n",
            "   0.05706005 0.05729144 0.0573982  0.05744674 0.05747003 0.05748231\n",
            "   0.05748913]]], shape=(1, 1, 19), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[0.04321738 0.0440757  0.04679358 0.04772776 0.04951145 0.04926932\n",
            "   0.04984847 0.05112995 0.05292444 0.05471902 0.05594329 0.0565308\n",
            "   0.05677111 0.05687065 0.05691254 0.05693046 0.05693844 0.05694205\n",
            "   0.05694362]]], shape=(1, 1, 19), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[0.03645559 0.03782028 0.0420422  0.04368374 0.04661271 0.04622972\n",
            "   0.04728515 0.04962162 0.05297343 0.05639457 0.05870584 0.05975569\n",
            "   0.06014985 0.06030171 0.06036339 0.06038936 0.06040085 0.06040604\n",
            "   0.06040825]]], shape=(1, 1, 19), dtype=float32)\n",
            "-\n",
            "Input sentence: ankne\n",
            "Decoded sentence: aanee\n",
            "\n",
            "tf.Tensor(\n",
            "[[[0.04406807 0.04403535 0.04662462 0.0462159  0.04542822 0.04619096\n",
            "   0.04631977 0.04766365 0.05023187 0.05351203 0.05624904 0.05785724\n",
            "   0.0587052  0.05915767 0.0593962  0.05952016 0.05958309 0.05961346\n",
            "   0.05962754]]], shape=(1, 1, 19), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[0.01526688 0.01531494 0.01983578 0.0192311  0.01804191 0.01985419\n",
            "   0.02045957 0.0237135  0.03114431 0.04514698 0.06321071 0.07689592\n",
            "   0.08444753 0.08841024 0.09048292 0.09155942 0.09210788 0.09237473\n",
            "   0.0925015 ]]], shape=(1, 1, 19), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[0.04436833 0.0447494  0.04722367 0.04750615 0.04730616 0.04860704\n",
            "   0.04913982 0.05053732 0.05249773 0.05448882 0.05591147 0.05666836\n",
            "   0.05703253 0.05721107 0.05729885 0.05734209 0.05736306 0.05737236\n",
            "   0.05737584]]], shape=(1, 1, 19), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[0.04857284 0.04890457 0.05014848 0.05050051 0.0505595  0.05134994\n",
            "   0.051694   0.0523466  0.05308637 0.05369306 0.05406479 0.05425215\n",
            "   0.05434253 0.05438676 0.05440801 0.05441802 0.05442264 0.05442445\n",
            "   0.05442484]]], shape=(1, 1, 19), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[0.04409185 0.04468839 0.04722189 0.0478665  0.04792856 0.04954015\n",
            "   0.05022589 0.05162261 0.05330659 0.05478595 0.05574748 0.05624853\n",
            "   0.05649476 0.05661951 0.05668322 0.05671552 0.05673156 0.05673897\n",
            "   0.05674208]]], shape=(1, 1, 19), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[0.0340673  0.03542321 0.0400201  0.04186591 0.04247501 0.04636952\n",
            "   0.04835616 0.05181551 0.05555189 0.05829699 0.05972618 0.06034964\n",
            "   0.0606254  0.06075697 0.06082181 0.06085392 0.06086942 0.06087627\n",
            "   0.06087874]]], shape=(1, 1, 19), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[0.03181627 0.03335681 0.03816998 0.0404035  0.04124733 0.045769\n",
            "   0.04819608 0.05229056 0.05656352 0.05944909 0.06074503 0.06122642\n",
            "   0.0614176  0.06150399 0.0615453  0.06156535 0.06157492 0.06157894\n",
            "   0.06158023]]], shape=(1, 1, 19), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[0.01937674 0.02092737 0.02685902 0.02956355 0.03052592 0.03705598\n",
            "   0.04078139 0.04788556 0.05649322 0.06341813 0.06719188 0.06886701\n",
            "   0.06962134 0.06999094 0.07017705 0.07027058 0.0703159  0.07033565\n",
            "   0.07034276]]], shape=(1, 1, 19), dtype=float32)\n",
            "-\n",
            "Input sentence: ankamaa\n",
            "Decoded sentence: maanaan\n",
            "\n",
            "tf.Tensor(\n",
            "[[[0.04406807 0.04403535 0.04662462 0.0462159  0.04542822 0.04619096\n",
            "   0.04631977 0.04766365 0.05023187 0.05351203 0.05624904 0.05785724\n",
            "   0.0587052  0.05915767 0.0593962  0.05952016 0.05958309 0.05961346\n",
            "   0.05962754]]], shape=(1, 1, 19), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[0.01526688 0.01531494 0.01983578 0.0192311  0.01804191 0.01985419\n",
            "   0.02045957 0.0237135  0.03114431 0.04514698 0.06321071 0.07689592\n",
            "   0.08444753 0.08841024 0.09048292 0.09155942 0.09210788 0.09237473\n",
            "   0.0925015 ]]], shape=(1, 1, 19), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[0.04436833 0.0447494  0.04722367 0.04750615 0.04730616 0.04860704\n",
            "   0.04913982 0.05053732 0.05249773 0.05448882 0.05591147 0.05666836\n",
            "   0.05703253 0.05721107 0.05729885 0.05734209 0.05736306 0.05737236\n",
            "   0.05737584]]], shape=(1, 1, 19), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[0.04857284 0.04890457 0.05014848 0.05050051 0.0505595  0.05134994\n",
            "   0.051694   0.0523466  0.05308637 0.05369306 0.05406479 0.05425215\n",
            "   0.05434253 0.05438676 0.05440801 0.05441802 0.05442264 0.05442445\n",
            "   0.05442484]]], shape=(1, 1, 19), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[0.04409185 0.04468839 0.04722189 0.0478665  0.04792856 0.04954015\n",
            "   0.05022589 0.05162261 0.05330659 0.05478595 0.05574748 0.05624853\n",
            "   0.05649476 0.05661951 0.05668322 0.05671552 0.05673156 0.05673897\n",
            "   0.05674208]]], shape=(1, 1, 19), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[0.0340673  0.03542321 0.0400201  0.04186591 0.04247501 0.04636952\n",
            "   0.04835616 0.05181551 0.05555189 0.05829699 0.05972618 0.06034964\n",
            "   0.0606254  0.06075697 0.06082181 0.06085392 0.06086942 0.06087627\n",
            "   0.06087874]]], shape=(1, 1, 19), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[0.03181627 0.03335681 0.03816998 0.0404035  0.04124733 0.045769\n",
            "   0.04819608 0.05229056 0.05656352 0.05944909 0.06074503 0.06122642\n",
            "   0.0614176  0.06150399 0.0615453  0.06156535 0.06157492 0.06157894\n",
            "   0.06158023]]], shape=(1, 1, 19), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[0.01937674 0.02092737 0.02685902 0.02956355 0.03052592 0.03705598\n",
            "   0.04078139 0.04788556 0.05649322 0.06341813 0.06719188 0.06886701\n",
            "   0.06962134 0.06999094 0.07017705 0.07027058 0.0703159  0.07033565\n",
            "   0.07034276]]], shape=(1, 1, 19), dtype=float32)\n",
            "-\n",
            "Input sentence: ankma\n",
            "Decoded sentence: maanaan\n",
            "\n",
            "tf.Tensor(\n",
            "[[[0.04406807 0.04403535 0.04662462 0.0462159  0.04542822 0.04619096\n",
            "   0.04631977 0.04766365 0.05023187 0.05351203 0.05624904 0.05785724\n",
            "   0.0587052  0.05915767 0.0593962  0.05952016 0.05958309 0.05961346\n",
            "   0.05962754]]], shape=(1, 1, 19), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[0.01526688 0.01531494 0.01983578 0.0192311  0.01804191 0.01985419\n",
            "   0.02045957 0.0237135  0.03114431 0.04514698 0.06321071 0.07689592\n",
            "   0.08444753 0.08841024 0.09048292 0.09155942 0.09210788 0.09237473\n",
            "   0.0925015 ]]], shape=(1, 1, 19), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[0.04436833 0.0447494  0.04722367 0.04750615 0.04730616 0.04860704\n",
            "   0.04913982 0.05053732 0.05249773 0.05448882 0.05591147 0.05666836\n",
            "   0.05703253 0.05721107 0.05729885 0.05734209 0.05736306 0.05737236\n",
            "   0.05737584]]], shape=(1, 1, 19), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[0.04857284 0.04890457 0.05014848 0.05050051 0.0505595  0.05134994\n",
            "   0.051694   0.0523466  0.05308637 0.05369306 0.05406479 0.05425215\n",
            "   0.05434253 0.05438676 0.05440801 0.05441802 0.05442264 0.05442445\n",
            "   0.05442484]]], shape=(1, 1, 19), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[0.04409185 0.04468839 0.04722189 0.0478665  0.04792856 0.04954015\n",
            "   0.05022589 0.05162261 0.05330659 0.05478595 0.05574748 0.05624853\n",
            "   0.05649476 0.05661951 0.05668322 0.05671552 0.05673156 0.05673897\n",
            "   0.05674208]]], shape=(1, 1, 19), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[0.0340673  0.03542321 0.0400201  0.04186591 0.04247501 0.04636952\n",
            "   0.04835616 0.05181551 0.05555189 0.05829699 0.05972618 0.06034964\n",
            "   0.0606254  0.06075697 0.06082181 0.06085392 0.06086942 0.06087627\n",
            "   0.06087874]]], shape=(1, 1, 19), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[0.03181627 0.03335681 0.03816998 0.0404035  0.04124733 0.045769\n",
            "   0.04819608 0.05229056 0.05656352 0.05944909 0.06074503 0.06122642\n",
            "   0.0614176  0.06150399 0.0615453  0.06156535 0.06157492 0.06157894\n",
            "   0.06158023]]], shape=(1, 1, 19), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[0.01937674 0.02092737 0.02685902 0.02956355 0.03052592 0.03705598\n",
            "   0.04078139 0.04788556 0.05649322 0.06341813 0.06719188 0.06886701\n",
            "   0.06962134 0.06999094 0.07017705 0.07027058 0.0703159  0.07033565\n",
            "   0.07034276]]], shape=(1, 1, 19), dtype=float32)\n",
            "-\n",
            "Input sentence: ankmaa\n",
            "Decoded sentence: maanaan\n",
            "\n",
            "tf.Tensor(\n",
            "[[[0.04406807 0.04403535 0.04662462 0.0462159  0.04542822 0.04619096\n",
            "   0.04631977 0.04766365 0.05023187 0.05351203 0.05624904 0.05785724\n",
            "   0.0587052  0.05915767 0.0593962  0.05952016 0.05958309 0.05961346\n",
            "   0.05962754]]], shape=(1, 1, 19), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[0.01526688 0.01531494 0.01983578 0.0192311  0.01804191 0.01985419\n",
            "   0.02045957 0.0237135  0.03114431 0.04514698 0.06321071 0.07689592\n",
            "   0.08444753 0.08841024 0.09048292 0.09155942 0.09210788 0.09237473\n",
            "   0.0925015 ]]], shape=(1, 1, 19), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[0.04436833 0.0447494  0.04722367 0.04750615 0.04730616 0.04860704\n",
            "   0.04913982 0.05053732 0.05249773 0.05448882 0.05591147 0.05666836\n",
            "   0.05703253 0.05721107 0.05729885 0.05734209 0.05736306 0.05737236\n",
            "   0.05737584]]], shape=(1, 1, 19), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[0.04857284 0.04890457 0.05014848 0.05050051 0.0505595  0.05134994\n",
            "   0.051694   0.0523466  0.05308637 0.05369306 0.05406479 0.05425215\n",
            "   0.05434253 0.05438676 0.05440801 0.05441802 0.05442264 0.05442445\n",
            "   0.05442484]]], shape=(1, 1, 19), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[0.04409185 0.04468839 0.04722189 0.0478665  0.04792856 0.04954015\n",
            "   0.05022589 0.05162261 0.05330659 0.05478595 0.05574748 0.05624853\n",
            "   0.05649476 0.05661951 0.05668322 0.05671552 0.05673156 0.05673897\n",
            "   0.05674208]]], shape=(1, 1, 19), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[0.0340673  0.03542321 0.0400201  0.04186591 0.04247501 0.04636952\n",
            "   0.04835616 0.05181551 0.05555189 0.05829699 0.05972618 0.06034964\n",
            "   0.0606254  0.06075697 0.06082181 0.06085392 0.06086942 0.06087627\n",
            "   0.06087874]]], shape=(1, 1, 19), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[0.03181627 0.03335681 0.03816998 0.0404035  0.04124733 0.045769\n",
            "   0.04819608 0.05229056 0.05656352 0.05944909 0.06074503 0.06122642\n",
            "   0.0614176  0.06150399 0.0615453  0.06156535 0.06157492 0.06157894\n",
            "   0.06158023]]], shape=(1, 1, 19), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[0.01937674 0.02092737 0.02685902 0.02956355 0.03052592 0.03705598\n",
            "   0.04078139 0.04788556 0.05649322 0.06341813 0.06719188 0.06886701\n",
            "   0.06962134 0.06999094 0.07017705 0.07027058 0.0703159  0.07033565\n",
            "   0.07034276]]], shape=(1, 1, 19), dtype=float32)\n",
            "-\n",
            "Input sentence: ankmaan\n",
            "Decoded sentence: maanaan\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-f04752c1fc2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mtest_acc_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-74-f04752c1fc2c>\u001b[0m in \u001b[0;36mtest_acc_attention\u001b[0;34m(m_name, latent_dim, n_e_layers, n_d_layers)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0minput_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq_index\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mseq_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mdecoded_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_sequence_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0matten\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdecoded_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m       \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-73-493f110c1c2d>\u001b[0m in \u001b[0;36mdecode_sequence_attention\u001b[0;34m(input_seq, encoder_model, decoder_model, decoder_dense, atten_layer)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mdecoded_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstop_condition\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mdec_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_seq\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstates_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0matten_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0matten_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menc_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_outputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1627\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1628\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1629\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1631\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    860\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}